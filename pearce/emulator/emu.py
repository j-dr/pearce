#!/bin/bash
"""The Emu object esentially wraps the George gaussian process code. It handles building, training, and predicting."""

import warnings
from glob import glob
from itertools import izip
from multiprocessing import cpu_count
from os import path
from time import time
from abc import ABCMeta, abstractmethod

import emcee as mc
import numpy as np
import scipy.optimize as op
import george
from george.kernels import *
from scipy.interpolate import interp1d, interp2d
from scipy.linalg import inv
from scipy.spatial import KDTree
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.svm import SVR

from .ioHelpers import global_file_reader, obs_file_reader
from .trainingData import GLOBAL_FILENAME


class Emu(object):
    '''Main Emulator base class. Cannot itself be instatiated; can only be accessed via subclasses.
       controls all loading, manipulation, and emulation of data.'''

    __metaclass__ = ABCMeta
    valid_methods = {'gp', 'svr', 'gbdt', 'rf', 'krr'}  # could add more, coud even check if they exist in sklearn

    def __init__(self, training_dir, method='gp', hyperparams={}, params=None, fixed_params={},
                 independent_variable=None):
        '''
        Initialize the Emu
        :param training_dir:
            Directory containing training data, in the format generated by trainingData.py
        :param method:
            Emulation method. Valid methods are 'gp', 'svr', 'gbdt', 'rf', and 'krr'. Default is 'gp'. GP is
            conducted by george, all others are executed by sklearn. Kernel based methods use a george kernel.
        :param hyperparams:
            Hyperparameters for the emulator. Gp hyperparams are passed into george, others are passed into
            sklearn. A special hyperparam is 'metric', which determines the metric for kernel-based methods.
             See documentation for a full list of hyperparameters.
             Default is {}.
        :param params:
            List of parameter namedtuples. Defines the order of the parameters in the data matrix.
             Default is None, where DEFAULT_PARAMS is loaded from ioHelpers
        :param fixed_params:
            Parameters to hold fixed during training. Key is the name of the param, and value is the fixed value.
             Default is {}.
        :param independent_variable:
            Indepent variable to emulate. Default is None, which just emulates the iv in the training data
            directly. Presently the only acceptable option is 'r2', which emulates r^2 times the
            parameter in the training data.
        '''
        # TODO don't need the user to pass in params. It can be generated from the traning data with an ordered_dict.
        # It should be somewhat hidden from the user.

        assert method in self.valid_methods

        if independent_variable == 'bias':
            raise NotImplementedError("I have to work on how to do xi_mm first.")
        assert independent_variable in {None, 'r2'}  # no bias for now.

        # TODO I hate the assembly bias parameter keys. It'd be nice if the use could pass in something
        # else and I make a change

        # use default if needed
        if params is None:
            from .ioHelpers import DEFAULT_PARAMS as params

        self.method = method
        # TODO ordered_params needs some checks, cuz i've hardcoded some index stuff in the new version
        # not necessary since it will be delted in a near future version.
        # or, be less lazy with those. (sure).
        # I'll probably just remove it in an update soon.
        self.ordered_params = params
        if any(p.name == 'z' for p in self.ordered_params) and any(p.name == 'r' for p in self.ordered_params):
            if self.ordered_params[-1].name != 'r' or self.ordered_params[-2].name != 'z':
                warnings.warn('Your ordered params order is possibly problematic! ')

        self.fixed_params = fixed_params
        self.independent_variable = independent_variable

        self.load_training_data(training_dir)
        self.build_emulator(hyperparams)

    ###Data Loading and Manipulation####################################################################################

    def get_data(self, data_dir, em_params, fixed_params, independent_variable):
        """
        Read data in the format compatible with this object and return it
        :param data_dir:
            Directory where data from trainingData is stored
        :param em_params:
            Parameters held fixed by the emulator. Slightly different than fixed_params, used for plotting.
        :param fixed_params:
            Parameters to hold fixed. Only available if data in data_dir is a full hypercube, not a latin hypercube
        :param independent_variable:
            Independant variable to emulate. Options are xi, r2xi, and bias (eventually)..
        :return: x, y, y_err, all numpy arrays.
                 x is (n_data_points, n_params)
                 y and y)err are (n_data_points, )
        """

        input_params = {}
        input_params.update(em_params)
        input_params.update(fixed_params)
        # TODO a more rigorous test would be better
        assert len(input_params) - len(self.ordered_params) <= 1  # can exclude r
        # HERE can we exclude z too, somehow?
        sub_dirs = glob(path.join(data_dir, 'a_*'))
        sub_dirs_as = np.array([float(fname[-7:]) for fname in sub_dirs])

        # sort them for consistancy
        sort_idxs = np.argsort(sub_dirs_as)
        sub_dirs_as = sub_dirs_as[sort_idxs]
        sub_dirs = [sub_dirs[i] for i in sort_idxs]

        if 'z' in fixed_params:  # don't have to look in dirs that aren't fixed.
            zs = fixed_params['z']
            if type(zs) is float:
                zs = [zs]
            sub_dirs = []
            _sub_dirs_as = []
            # check the fixed z's against the a's we have in training data
            # only keep dirs that match
            for z in zs:
                input_a = 1 / (1 + z)
                idx = np.argmin(np.abs(sub_dirs_as - input_a))
                a = sub_dirs_as[idx]
                if np.abs(a - input_a) > 0.05:  # tolerance
                    raise IOError('No subfolder within tolerance of z=%.3f' % z)
                sub_dirs.append(path.join(data_dir, 'a_%.5f' % a))
                _sub_dirs_as.append(a)
            sub_dirs_as = np.array(_sub_dirs_as)
        # we store redshifts, not scale factors
        self.redshift_bin_centers = 1 / sub_dirs_as - 1

        all_x, all_y, all_yerr = [], [], []

        #TODO need a dictionary of files to refer to 
        for sub_dir, z in zip(sub_dirs, self.redshift_bin_centers):

            bins, cosmo_params, obs, sampling_method = global_file_reader(path.join(sub_dir, GLOBAL_FILENAME))
            # Could add an assert that a = cosmo_params['scale_factor']
            if fixed_params and sampling_method == 'LHC':
                if fixed_params.keys() != ['z']:  # allowed:
                    raise ValueError('Fixed parameters is not empty, but the data in data_dir is form a Latin Hypercube. \
                                    Cannot performs slices on a LHC.')

            self.obs = obs

            # sorted to ensure parameters line up correctly
            obs_files = sorted(glob(path.join(sub_dir, 'obs*.npy')))
            cov_files = sorted(glob(path.join(sub_dir, 'cov*.npy')))

            # store the binning for the scale_bins
            # assumes that it's the same for each box, which should be true.
            self.scale_bin_centers = (bins[:-1] + bins[1:]) / 2
            scale_nbins = self.scale_bin_centers.shape[0]

            # TODO it is at this stage we assume r is last in observed params. Make sure this is explicit.

            npoints = len(obs_files) * scale_nbins  # each file contains NBINS points in r, and each file is a 6-d point

            # parameters that are not held fixed
            varied_params = set([p.name for p in self.ordered_params]) - set(fixed_params.keys())

            ndim = len(varied_params)
            x = np.zeros((npoints, ndim))
            y = np.zeros((npoints,))
            yerr = np.zeros((npoints,))

            warned = False
            num_skipped = 0
            num_used = 0

            for idx, (obs_file, cov_file) in enumerate(izip(obs_files, cov_files)):
                params, obs, cov = obs_file_reader(obs_file, cov_file)

                # skip values that aren't where we've fixed them to be.
                # It'd be nice to do this before the file I/O. Not possible without putting all info in the filename.
                # or, a more nuanced file structure
                # Note is is complex because fixed_params can have floats or arrays of floats.

                # TODO I can fix the above problem with some kind of global file. It'd be easy to do!

                # TODO check if a fixed_param is not one of the options i.e. typo
                to_continue = True
                for key, val in input_params.iteritems():
                    if key in {'z', 'r'}:
                        continue  # these won't be in params, or have been already screened.
                    if type(val) is type(y):  # array
                        if np.all(np.abs(params[key] - val) > 1e-3):
                            break
                    elif np.abs(params[key] - val) > 1e-3:  # float
                        break
                else:
                    to_continue = False  # no break. We found a parameter not in our fixed values

                # skip NaNs as well.
                if np.any(np.isnan(cov)) or np.any(np.isnan(obs)):
                    if not warned:
                        warnings.warn('WARNING: NaN detected. Skipping point in %s' % cov_file)
                        warned = True
                    num_skipped += 1
                    to_continue = True

                if to_continue:
                    continue

                num_used += 1

                # doing some shuffling and stacking
                file_params = []
                # NOTE could do a param ordering here
                for p in self.ordered_params:
                    if p.name in fixed_params:  # may need to be input_params
                        continue
                    # TODO if I have my own defined order internally I can do this at the end outside the loop
                    # TODO change 'r' to something else.
                    if p.name == 'r':
                        file_params.append(np.log10(self.scale_bin_centers))
                    elif p.name == 'z':
                        file_params.append(np.ones((scale_nbins,)) * z)
                    else:
                        file_params.append(np.ones((scale_nbins,)) * params[p.name])

                x[idx * scale_nbins:(idx + 1) * scale_nbins, :] = np.stack(file_params).T

                y[idx * scale_nbins:(idx + 1) * scale_nbins], yerr[idx * scale_nbins:( \
                                                                                         idx + 1) * scale_nbins] = self._iv_transform(
                    independent_variable, obs, cov)

            # remove rows that were skipped due to the fixed thing
            # NOTE: HACK
            # a reshape may be faster.
            # TODO could I use Nonzero, or, better, keep track of the index I need?? 
            zeros_slice = np.any(x != 0.0, axis=1)

            all_x.append(x[zeros_slice])
            all_y.append(y[zeros_slice])
            all_yerr.append(yerr[zeros_slice])

        # TODO sort?
        return np.vstack(all_x), np.hstack(all_y), np.hstack(all_yerr)

    def get_plot_data(self, em_params, training_dir, independent_variable=None, fixed_params={},
                      dependent_variable='r'):
        """
        Similar function to load_training_data. However, returns values for plotting comparisons to the emulator.
        :param em_params:
            Similar to fixed params. A dictionary of values held fixed in the emulator, as opposed to fixed_params
            which are values held fixed in the training data.
        :param training_dir:
            Directory where training data from trainginData is stored.
        :param independent_variable:
            Independant variable to emulate. Options are xi, r2xi, and bias (eventually).
        :param fixed_params:
            Parameters to hold fixed. Only available if data in training_dir is a full hypercube, not a latin hypercube.
        :param dependent_variable:
            "x axis" variable. Default is 'r', but 'z' is also acceptable.
        :return: log_r (or z), y, yerr for the independent variable at the points specified by fixed nad em params.
        """

        x, y, yerr = self.get_data(training_dir, em_params, fixed_params, independent_variable)

        sort_idxs = self._sort_params(x, argsort=True)

        log_bin_centers = np.log10(self.scale_bin_centers) if dependent_variable == 'r' else self.redshift_bin_centers
        # repeat for each row of y
        log_bin_centers = np.tile(log_bin_centers, sort_idxs.shape[0] / len(log_bin_centers))

        # TODO do I want to reshape to y.shape((-1, log_bin_centers.shape(1)) ?
        return log_bin_centers, y[sort_idxs], yerr[sort_idxs]

    @abstractmethod
    def load_training_data(self, training_dir):
        pass

    def _iv_transform(self, independent_variable, obs, cov):
        """
        Independent variable tranform. Helper function that consolidates this operation all in one place.
        :param independent_variable:
            Which iv to transform to. Current optins are None (just take log) and r2.
        :param obs:
            Observable to transform (xi, wprp, etc.)
        :param cov:
            Covariance of obs
        :return:
            y, yerr the transformed iv's for the emulator
        """
        if independent_variable is None:
            y = np.log10(obs)
            # Approximately true, may need to revisit
            # yerr[idx * NBINS:(idx + 1) * NBINS] = np.sqrt(np.diag(cov)) / (xi * np.log(10))
            y_err = np.sqrt(np.diag(cov)) / (obs * np.log(10))
        elif independent_variable == 'r2':  # r2xi
            y = obs * self.scale_bin_centers * self.scale_bin_centers
            y_err = np.sqrt(np.diag(cov)) * self.scale_bin_centers
        else:
            raise ValueError('Invalid independent variable %s' % independent_variable)

        """
        if independent_variable == 'bias':
            y[idx * NBINS:(idx + 1) * NBINS] = xi / xi_mm
            ycovs.append(cov / np.outer(xi_mm, xi_mm))
        """

        return y, y_err

    def _sort_params(self, t, argsort=False):
        """
        Sort the parameters in a defined away given the orderering.
        :param t:
            Parameter vector to sort. Should have dims (N, N_params) and be in the order
            defined by ordered_params
        :param argsort:
            If true, return indicies that would sort the array rather than the sorted array itself.
            Default is False.
        :return:
            If not argsort, returns the sorted array by column and row. 
            If argsort, return the indicies that would sort the array.
        """
        if t.shape[0] == 1:
            if argsort:
                return np.array([0])
            return t  # a single row array is already sorted!

        if argsort:  # returns indicies that would sort the array
            # weird try structure because this view is very tempermental!
            try:
                idxs = np.argsort(t.view(','.join(['float64' for _ in xrange(min(t.shape))])),
                                  order=['f%d' % i for i in xrange(min(t.shape))], axis=0)
            except ValueError:  # sort with other side
                idxs = np.argsort(t.view(','.join(['float64' for _ in xrange(max(t.shape))])),
                                  order=['f%d' % i for i in xrange(max(t.shape))], axis=0)

            return idxs[:, 0]

        try:
            t = np.sort(t.view(','.join(['float64' for _ in xrange(min(t.shape))])),
                        order=['f%d' % i for i in xrange(min(t.shape))], axis=0).view(np.float)
        except ValueError:  # sort with other side
            t = np.sort(t.view(','.join(['float64' for _ in xrange(max(t.shape))])),
                        order=['f%d' % i for i in xrange(max(t.shape))], axis=0).view(np.float)

        return t

    ###Emulator Building and Training###################################################################################

    def build_emulator(self, hyperparams):
        """
        Initialization of the emulator from recovered training data. Calls submethods depending on "method"
        :param hyperparams
            A dictionary of hyperparameter kwargs for the emulator
        :return: None
        """

        if self.method == 'gp':
            self._build_gp(hyperparams)
        else:  # an sklearn method
            self._build_skl(hyperparams)

    @abstractmethod
    def _build_gp(self, hyperparams):
        pass

    @abstractmethod
    def _build_skl(self, hyperparams):
        pass

    def _get_initial_guess(self, independent_variable):
        """
        Return the initial guess of the metric for the emulator, based on what the iv is. Guesses are learned from
        previous experiments.
        :param independent_variable:
            Which variable to return the guesses for.
        :return: initial_guesses, a dictionary of the guess for each parameter
        """

        # default
        ig = {'amp': 1}
        ig.update({p.name: 0.1 for p in self.ordered_params})

        if self.obs == 'xi':
            if independent_variable is None:
                ig = {'amp': 0.481, 'logMmin': 0.1349, 'sigma_logM': 0.089,
                      'logM0': 2.0, 'logM1': 0.204, 'alpha': 0.039,
                      'f_c': 0.041, 'r': 0.040, 'z': 1.0}
            else:
                # could have other guesses for this case, but don't have any now
                # leave this structure in case I make more later
                pass
        elif self.obs == 'wp':
            if independent_variable is None:
                ig = {'logMmin': 1.7348042925, 'f_c': 0.327508062386, 'logM0': 15.8416094906,
                      'sigma_logM': 5.36288382789, 'alpha': 3.63498762588, 'r': 0.306139450843,
                      'logM1': 1.66509412286, 'amp': 1.18212664544, 'z': 1.0}
        else:
            pass  # no other guesses saved yet.

        # remove entries for variables that are being held fixed.
        for key in self.fixed_params.iterkeys():
            try:
                del ig[key]
            except KeyError:
                pass  # can happen for redshift and others.

        return ig

    def _make_kernel(self, metric):
        """
        Helper method to build a george kernel for GP's and kernel-based regressions.
        :param metric:
            Hyperparams for kernel determining relative length scales and amplitudes
        :return:
            A george ExpSquredKernel object with this metric
        """

        if not metric:
            ig = self._get_initial_guess(self.independent_variable)
        else:
            ig = metric  # use the user's initial guesses

        metric = [ig['amp']]
        for p in self.ordered_params:
            if p.name in self.fixed_params:
                continue
            try:
                metric.append(ig[p.name])
            except KeyError:
                raise KeyError('Key %s was not in the metric.' % p.name)

        metric = np.array(metric)

        a = metric[0]
        # TODO other kernels?
        return a * ExpSquaredKernel(metric[1:], ndim=self.emulator_ndim)

    ###Emulation and methods that Utilize it############################################################################
    def emulate(self, em_params, gp_errs=False):
        """
        Perform predictions with the emulator.
        :param em_params:
            Dictionary of what values to predict at for each param. Values can be
            an array or a float.
        :param gp_errs:
            Boolean, decide whether or not to return the errors from the gp prediction. Default is False.
            Will throw error if method is not gp.
        :return: mu, (errs)
                  The predicted value and the uncertainties for the predictions
                  mu and errs both have shape (npoints,)
        """

        # only has meaning for gp's
        assert not gp_errs or self.method == 'gp'

        input_params = {}
        input_params.update(self.fixed_params)
        input_params.update(em_params)
        # TODO this check is insufficient
        # TODO make a helper function to include these sanity checks! 
        assert len(input_params) - self.emulator_ndim + self.fixed_ndim <= 2  # check dimenstionality
        for i in input_params:  # check that the names in input params are all defined in the ordering.
            assert any(i == p.name for p in self.ordered_params)
        # check that all params are defiend! 
        for p in self.ordered_params:
            assert p.name in input_params

        # i'd like to remove 'r'. possibly requiring a passed in param?
        t_list = [input_params[p.name] for p in self.ordered_params if p.name in em_params]
        t_grid = np.meshgrid(*t_list)
        t = np.stack(t_grid).T
        t = t.reshape((-1, self.emulator_ndim))

        # TODO george can sort?
        t = self._sort_params(t)

        return self._emulate_helper(t, gp_errs)

    @abstractmethod
    def _emulate_helper(self, t, gp_errs=False):
        pass

    def emulate_wrt_r(self, em_params, r_bin_centers, gp_errs=False):
        """
        Helper function to emulate over r bins.
        :param em_params:
            Parameters to predict at
        :param r_bin_centers:
            Radial bins to predict at
        :param gp_err:
            Boolean, whether or not to use the errors from the GP. Default is False.
            If method is not 'gp', will throw an error
        :return: mu, (errs)
                mu has shape (n_points, len(r_bin_centers))
                errs, if returned, has the same shape
        """
        ep = {}
        ep.update(em_params)
        z_bin_centers = np.array([])
        try:
            z_bin_centers = ep['z']
            if type(z_bin_centers) is float:
                z_bin_centers = np.array([z_bin_centers])
            del ep['z']
        except KeyError:
            pass

        out = self.emulate_wrt_r_z(ep, r_bin_centers, z_bin_centers, gp_errs)
        # now, reshape to have shape (-1, len(r_bin_centers))

        # Extract depending on if there are errors
        if gp_errs:
            _mu, _errs = out
        else:
            _mu = out

        mu = _mu.reshape((-1, r_bin_centers.shape[0]))
        if not gp_errs:
            return mu

        errs = _errs.reshape(mu.shape)
        return mu, errs

    def emulate_wrt_z(self, em_params, z_bin_centers, gp_errs=False):
        """
        Helper function to emulate over z bins.
        :param em_params:
            Parameters to predict at
        :param z_bin_centers:
            Redshift bins to predict at
        :param gp_err:
            Boolean, whether or not to use the errors from the GP. Default is False.
            If method is not 'gp', will throw an error
        :return:mu, (errs)
                mu has shape (n_points, len(z_bin_centers))
                errs, if returned, has the same shape
        """
        ep = {}
        ep.update(em_params)
        r_bin_centers = np.array([])
        try:
            r_bin_centers = ep['r']
            if type(r_bin_centers) is float:
                r_bin_centers = np.array([r_bin_centers])
            del ep['r']
        except KeyError:
            pass

        out = self.emulate_wrt_r_z(ep, r_bin_centers, z_bin_centers, gp_errs)
        # now, reshape to have shape (-1, len(z_bin_centers))
        if gp_errs:
            mu, errs = out
        else:
            mu = out
        # The swapaxes are necessary to make sure the reshape works properly
        mu = mu.swapaxes(1, 2).reshape((-1, z_bin_centers.shape[0]))

        if not gp_errs:
            return mu
        errs = errs.swapaxes(1, 2).reshape(mu.shape)
        return mu, errs

    def emulate_wrt_r_z(self, em_params, r_bin_centers, z_bin_centers, gp_errs=False):
        """
        Conveniance function. Add's 'r' to the emulation automatically, as this is the
        most common use case.
        :param em_params:
            Dictionary of what values to predict at for each param. Values can be array
            or float.
        :param bin_centers:
            Centers of bins to predict at, for each point in HOD-space.
        :return: mu, errs (if gp_errs == True)
                both have been reshaped to have shape (-1, len(z_bin_centers), len(r_bin_centers))
        """
        vep = dict(em_params)
        # take the log of r_bin_centers
        rpc = np.log10(r_bin_centers) if np.any(r_bin_centers) else np.array([])  # make sure not to throw an error
        # TODO change 'r' to something more general
        for key, val in zip(['r', 'z'], (rpc, z_bin_centers)):
            if key not in vep and val.size:  # key must not already exist and must be nonzero:
                vep[key] = val
        # vep.update({'r': np.log10(r_bin_centers), 'z': z_bin_centers})
        out = self.emulate(vep, gp_errs)
        if gp_errs:
            mu, errs = out
        else:
            mu = out
        mu = mu.reshape((-1, z_bin_centers.shape[0], r_bin_centers.shape[0]))
        if not gp_errs:
            return mu
        errs = errs.reshape(mu.shape)
        return mu, errs


    # TODO Jeremey keeps konwn uncertainties, I should do the same here, or near to here. 
    def estimate_uncertainty(self, truth_dir, N=None):
        """
        Estimate the uncertainty of the emulator by comparing to a "test" box of true values.
        :param truth_dir:
            Name of a directory of true test values, of the same format as the train_dir
        :param N:
            Number of points to compare to. If None (default) will use all points. Else will select random sample.
        :return:
            covariance matrix with dim n_binsxn_bins. Will only have diagonal elemtns of est. uncertainties.
        """
        rms_err = self.goodness_of_fit(truth_dir, N, statistic='rms')

        return np.diag(rms_err ** 2)

    # only predicts wrt r. don't know if that's an issue.
    def goodness_of_fit(self, truth_dir, N=None, statistic='r2'):
        """
        Calculate the goodness of fit of an emulator as compared to some validation data.
        :param truth_dir:
            Directory structured similarly to the training data, but NOT used for training.
        :param N:
            Number of points to use to calculate G.O.F. measures. "None" tests against all values in truth_dir. If N
            is less than the number of points, N are randomly selected. Default is None.
        :param statistic:
            What G.O.F. statistic to calculate. Default is r2. Other options are rmsfd, abs(olute), and rel(ative).
        :return: values, a numpy arrray of the calculated statistics at each of the N training points.
        """
        assert statistic in {'r2', 'rms', 'rmsfd', 'abs', 'log_abs', 'frac', 'log_frac'}
        if N is not None:
            assert N > 0 and int(N) == N

        x, y, _ = self.get_data(truth_dir, {}, self.fixed_params, self.independent_variable)

        bins, _, _, _ = global_file_reader(path.join(truth_dir, GLOBAL_FILENAME))
        bin_centers = (bins[1:] + bins[:-1]) / 2
        scale_nbins = len(bin_centers)

        # this hack is not a future proof test!
        # TODO this whole method needs adjusting. It doesn't work right for other subclasses.
        # The fix might be to have a helper method that does the proper reshaping done while loading data.
        if self.ordered_params[-1].name != 'r':
            x = x[0:-1:scale_nbins, :]

        y = y.reshape((-1, scale_nbins))

        np.random.seed(int(time()))

        if N is not None:  # make a random choice
            idxs = np.random.choice(x.shape[0], N, replace=False)

            x, y = x[idxs], y[idxs]

        pred_y = self._emulate_helper(x, False)
        pred_y = pred_y.reshape((-1, scale_nbins))

        # have to inerpolate...
        # TODO also have to fix when ordered is changed.
        # This is a question of waht subclass we're using in a sense.
        # this part could maybe just be wrapped into the above as a helper move.
        # I'd have to standarize the shaping and stuff...
        if self.ordered_params[-1].name != 'r' and not np.all(bin_centers == self.scale_bin_centers):
            bin_centers = bin_centers[self.scale_bin_centers[0] <= bin_centers <= self.scale_bin_centers[-1]]
            new_mu = []
            for mean in pred_y:
                xi_interpolator = interp1d(self.scale_bin_centers, mean, kind='slinear')
                interp_mean = xi_interpolator(bin_centers)
                new_mu.append(interp_mean)
            pred_y = np.array(new_mu)
            y = y[:, self.scale_bin_centers[0] <= bin_centers <= self.scale_bin_centers[-1]]

        if statistic == 'rmsfd':
            return np.sqrt(np.mean((((pred_y - y) ** 2) / (y ** 2)), axis=0))

        elif statistic == 'rms':
            return np.sqrt(np.mean(((pred_y - y) ** 2), axis=0))

        # TODO sklearn methods can do this themselves. But i've already tone the prediction!
        elif statistic == 'r2':  # r2
            SSR = np.sum((pred_y - y) ** 2, axis=0)
            SST = np.sum((y - y.mean(axis=0)) ** 2, axis=0)

            return 1 - SSR / SST

        elif statistic == 'abs':
            return 10 ** pred_y - 10 ** y
        elif statistic == 'log_abs':
            return pred_y - y
            # return np.mean((pred_y - y), axis=0)
        elif statistic == 'log_frac':  # 'rel'
            return (pred_y - y) / y
            # return np.mean((pred_y - y) / y, axis=0)
        else:  # 'frac'
            return (10 ** pred_y - 10 ** y) / (10 ** y)

    @abstractmethod
    def train_metric(self, **kwargs):
        pass

    # TODO this feature is not super useful anymore, and also is poorly defined w.r.t non gp methods.
    # did a lot of work on it tho, maybe i'll leave it around...?
    # TODO this feature is no longer correct with EC
    def _loo_errors(self, y, t):
        """
        Calculate the LOO Jackknife error matrix. This is implemented using the analytic LOO procedure,
        which is much faster than re-doing an inversion for each sample. May be useful if the GP's matrix is not
        accurate.
        :param y:
            Values of the independent variable for the training points, used in the prediction.
        :param t:
            Values of the dependant variables to predict at.
        :return:
            jk_cov: a covariance matrix with the dimensions of cov.
        """
        # from time import time

        assert self.method == 'gp'

        if isinstance(self, ExtraCrispy):
            emulator = self.emulators[0]  # hack for EC, do somethign smarter later
        else:
            emulator = self.emulator

        # We need to perform one full inverse to start.
        K_inv_full = emulator.solver.apply_inverse(np.eye(emulator._alpha.size),
                                                   in_place=True)

        # TODO deepcopy?
        x = self.x[:]

        N = K_inv_full.shape[0]

        mus = np.zeros((N, t.shape[0]))
        # t0 = time()

        # iterate over training points to leave out
        for idx in xrange(N):
            # swap the values of the LOO point and the last point.
            x[[N - 1, idx]] = x[[idx, N - 1]]
            y[[N - 1, idx]] = y[[idx, N - 1]]

            K_inv_full[[idx, N - 1], :] = K_inv_full[[N - 1, idx], :]
            K_inv_full[:, [idx, N - 1]] = K_inv_full[:, [N - 1, idx]]

            # the inverse of the LOO GP
            # formula found via MATH
            K_m_idx_inv = K_inv_full[:N - 1, :][:, :N - 1] \
                          - np.outer(K_inv_full[N - 1, :N - 1], K_inv_full[:N - 1, N - 1]) / K_inv_full[N - 1, N - 1]

            alpha_m_idx = np.dot(K_m_idx_inv, y[:N - 1] - emulator.mean(x[:N - 1]))

            Kxxs_t = emulator.kernel.value(t, x[:N - 1])

            # Store the estimate for this LOO GP
            mus[idx, :] = np.dot(Kxxs_t, alpha_m_idx) + emulator.mean(t)

            # restore the original values for the next loop
            x[[N - 1, idx]] = x[[idx, N - 1]]
            y[[N - 1, idx]] = y[[idx, N - 1]]

            K_inv_full[[idx, N - 1], :] = K_inv_full[[N - 1, idx], :]
            K_inv_full[:, [idx, N - 1]] = K_inv_full[:, [N - 1, idx]]

        # return the jackknife cov matrix.
        cov = (N - 1.0) / N * np.cov(mus, rowvar=False)
        if mus.shape[1] == 1:
            return np.array([[cov]])  # returns float in this case
        else:
            return cov

    # TODO move this to another module
    def run_mcmc(self, y, cov, bin_centers, nwalkers=1000, nsteps=100, nburn=20, n_cores='all'):
        """
        Run an MCMC sampler, using the emulator. Uses emcee to perform sampling.
        :param y:
            A true y value to recover the parameters of theta. NOTE: The emulator emulates some indepedant variables in 
            log space, others in linear. Make sure y is in the same space!
        :param cov:
            The measurement covariance matrix of y
        :param bin_centers:
            The centers of the bins y is measured in (radial or angular).
        :param nwalkers:
            Optional. Number of walkers for emcee. Default is 1000.
        :param nsteps:
            Optional. Number of steps for emcee. Default is 100.
        :param nburn:
            Optional. Number of burn-in steps for emcee. Default is 20.
        :param n_cores:
            Number of cores, either an iteger or 'all'. Default is 'all'.
        :return:
            chain, a numpy array of the sample chain.
        """

        assert n_cores == 'all' or n_cores > 0
        if type(n_cores) is not str:
            assert int(n_cores) == n_cores

        max_cores = cpu_count()
        if n_cores == 'all':
            n_cores = max_cores
        elif n_cores > max_cores:
            warnings.warn('n_cores invalid. Changing from %d to maximum %d.' % (n_cores, max_cores))
            n_cores = max_cores
            # else, we're good!

        assert y.shape[0] == cov.shape[0] and cov.shape[1] == cov.shape[0]
        assert y.shape[0] == bin_centers.shape[0]

        sampler = mc.EnsembleSampler(nwalkers, self.sampling_ndim, lnprob,
                                     threads=n_cores, args=(self, y, cov, bin_centers))

        pos0 = np.zeros((nwalkers, self.sampling_ndim))
        # The zip ensures we don't use the params that are only for the emulator
        for idx, (p, _) in enumerate(izip(self.ordered_params, xrange(self.sampling_ndim))):
            # pos0[:, idx] = np.random.uniform(p.low, p.high, size=nwalkers)
            pos0[:, idx] = np.random.normal(loc=(p.high + p.low) / 2, scale=(p.high + p.low) / 10, size=nwalkers)

        sampler.run_mcmc(pos0, nsteps)

        # Note, still an issue of param label ordering here.
        chain = sampler.chain[:, nburn:, :].reshape((-1, self.sampling_ndim))

        return chain


# These functions cannot be instance methods
# Emcee throws a fit when trying to compile the liklihood functions that are attached
# to the object calling it
def lnprob(theta, *args):
    """
    The total liklihood for an MCMC. Sadly, can't be an instance of the Emu Object.
    :param theta:
        Parameters for the proposal
    :param args:
        Arguments to pass into the liklihood
    :return:
        Log Liklihood of theta, a float.
    """
    lp = lnprior(theta, *args)
    if not np.isfinite(lp):
        return -np.inf
    return lp + lnlike(theta, *args)


def lnprior(theta, emu, *args):
    """
    Prior for an MCMC. Currently asserts theta is between the boundaries used to make the emulator.
    Could do something more clever later.
    :param theta:
        The parameters proposed by the sampler.
    :param emu:
        The emulator object. Needs to be accessed to get the priors.
    :return:
        Either 0 or -np.inf, depending if the params are allowed or not.
    """
    return 0 if all(p.low < t < p.high for p, t in izip(emu.ordered_params, theta)) else -np.inf


def lnlike(theta, emu, y, cov, bin_centers):
    """
    The liklihood of parameters theta given the other parameters and the emulator.
    :param theta:
        Proposed parameters.
    :param emu:
        The emulator object. Used to perform the emulation.
    :param y:
        The measured value of the observable to compare to the emulator.
    :param cov:
        The covariance matrix of the measured values.
    :param bin_centers:
        The centers of the bins y is measured in, angular or radial.
    :return:
        The log liklihood of theta given the measurements and the emulator.
    """
    em_params = {p.name: t for p, t in zip(emu.ordered_params, theta)}

    # using my own notation
    y_bar, G = emu.emulate_wrt_r(em_params, bin_centers)
    # should chi2 be calculated in log or linear?
    # answer: the user is responsible for taking the log before it comes here.

    D = G + cov
    delta = y_bar - y
    chi2 = -0.5 * np.dot(delta, np.dot(inv(D), delta))
    return chi2


class OriginalRecipe(Emu):
    """Emulator that emulates with bins as an implicit parameter. """

    def load_training_data(self, training_dir):
        """
        Read the training data for the emulator and attach it to the object.
        :param training_dir:
            Directory where training data from trainginData is stored. May also be a list of several points.
        :param fixed_params:
            Parameters to hold fixed. Only available if data in training_dir is a full hypercube, not a latin hypercube.
        :return: None
        """
        if type(training_dir) is not list:
            training_dir = [training_dir]

        xs, ys, yerrs = [], [], []
        for td in training_dir:
            x, y, yerr = self.get_data(td, {}, self.fixed_params, self.independent_variable)
            xs.append(x)
            ys.append(y)
            yerrs.append(yerr)

        self.x = np.vstack(xs)
        # hstack for 1-D
        self.y = np.hstack(ys)
        self.yerr = np.hstack(yerrs)

        self.y_hat = np.zeros(self.y.shape[1]) if len(y.shape) > 1 else 0  # self.y.mean(axis = 0)
        self.y -= self.y_hat

        ndim = self.x.shape[1]
        self.fixed_ndim = len(self.fixed_params)
        self.emulator_ndim = ndim  # The number of params for the emulator is different than those in sampling.

    def _build_gp(self, hyperparams):
        """
        Initialize the GP emulator.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        # TODO could use more of the hyperparams...
        metric = hyperparams['metric'] if 'metric' in hyperparams else {}
        kernel = self._make_kernel(metric)
        # TODO is it confusing for this to have the same name as the sklearn object with a different API?
        # maybe it should be a property? or private?
        # The user should never access it directly, I think. They shoud only call the emu functions.
        self.emulator = george.GP(kernel)
        # gp = george.GP(kernel, solver=george.HODLRSolver, nleaf=x.shape[0]+1,tol=1e-18)

        self.emulator.compute(self.x, self.yerr, sort=False)  # NOTE I'm using a modified version of george!

    def _build_skl(self, hyperparams):
        """
        Build a scikit learn emulator
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        skl_methods = {'gbdt': GradientBoostingRegressor, 'rf': RandomForestRegressor, \
                       'svr': SVR, 'krr': KernelRidge}

        if self.method in {'svr', 'krr'}:  # kernel based method
            metric = hyperparams['metric'] if 'metric' in hyperparams else {}
            kernel = self._make_kernel(metric)
            if 'metric' in hyperparams:
                del hyperparams['metric']
            # slight difference in use for these saldy
            if self.method == 'svr':
                hyperparams['kernel'] = kernel.value
            else:  # krr
                hyperparams['kernel'] = lambda x1, x2: kernel.value(np.array([x1]), np.array([x2]))

        self.emulator = skl_methods[self.method](**hyperparams)
        self.emulator.fit(self.x, self.y)

    def _emulate_helper(self, t, gp_errs):
        """
        Helper function that takes a dependent variable matrix and makes a prediction.
        :param t:
            Dependent variable matrix. Assumed to be in the order defined by ordered_params
        :param gp_errs:
            Whether or not to return errors in the gp case
        :return:
            mu, err (if gp_errs True). Predicted value for dependetn variable t.
            mu and err both have shape (t.shape[0])
        """
        if self.method == 'gp':
            if gp_errs:
                mu, cov = self.emulator.predict(self.y, t, mean_only=False)
                return mu, np.diag(cov)
            else:
                return self.emulator.predict(self.y, t, mean_only=True)
        else:
            return self.emulator.predict(t)
    
    def train_metric(self, **kwargs):
        """
        Train the metric parameters of the GP. Has a spotty record of working.
        Best used as used in lowDimTraining.
        If attempted to be used with an emulator that is not GP, will raise an error.
        :param kwargs:
            Kwargs that will be passed into the scipy.optimize.minimize
        :return: success: True if the training was successful.
        """

        # TODO kernel based methods may want to use this...
        assert self.method == 'gp'

        # move these outside? hm.
        def nll(p):
            # Update the kernel parameters and compute the likelihood.
            # params are log(a) and log(m)
            self.emulator.kernel[:] = p
            ll = self.emulator.lnlikelihood(self.y, quiet=True)

            # The scipy optimizer doesn't play well with infinities.
            return -ll if np.isfinite(ll) else 1e25

        # And the gradient of the objective function.
        def grad_nll(p):
            # Update the kernel parameters and compute the likelihood.
            self.emulator.kernel[:] = p
            return -self.emulator.grad_lnlikelihood(self.y, quiet=True)

        p0 = self.emulator.kernel.vector
        results = op.minimize(nll, p0, jac=grad_nll, **kwargs)
        # results = op.minimize(nll, p0, jac=grad_nll, method='TNC', bounds =\
        #   [(np.log(0.01), np.log(10)) for i in xrange(ndim+1)],options={'maxiter':50})
        print results

        self.emulator.kernel[:] = results.x
        self.emulator.recompute()
        # self.metric = np.exp(results.x)

        return results.success

def get_leaves(kdtree):
    """
    Helper function for recursively retriving the leaves of a KDTree
    :param: kdtree
        instance of KDTree to recover leaves of.
    :return: leaves, a list of  numpy arrays of shape (experts, points_per_expert), of leaves. 
    """
    points_per_expert= kdtree.leafsize
    experts = kdtree.n/points_per_expert
    leaves_list = []
    get_leaves_helper(kdtree.tree,leaves_list)

    return [np.array(l) for l in leaves_list] 

def get_leaves_helper(node, leaves):
    """
    Meta helper function. Recursively 
    """
    if isinstance(node, KDTree.leafnode):
        leaves.append(node.idx)
    else:
        get_leaves_helper(node.less, leaves)
        get_leaves_helper(node.greater, leaves)

class ExtraCrispy(Emu):
    """Emulator that emulates with a mixture of expert learners rather than a single one."""

    def __init__(self, training_dir, experts, overlap=1, partition_scheme='kdtree', **kwargs):
        """
        Similar initialization as the superclass with one additional parameter: Em_param
        :param training_dir:
            See above in EMu
        :param experts:
            number of experts to use in the mixture. Must be an integer greater than 1.
        :param overlap:
            overlap in training points between experts. For example, if overlap=2, each datapoint will be in 
            2 experts. Default is 1, no overlap. 
        :param kwargs:
            As in Emu
        """

        assert experts > 1  and int(experts) == experts#no point in having less than this
        # TODO experts max value?
        #no point in having overlap the same as experts. You just have experts-many identical gps!  
        assert experts > overlap > 0 and int(overlap) == overlap
        assert partition_scheme in {'kdtree', 'random'}

        self.experts = int(experts)
        self.overlap = int(overlap)
        self.partition_scheme = partition_scheme

        super(ExtraCrispy, self).__init__(training_dir, **kwargs)

    def load_training_data(self, training_dir):
        """
        Read the training data for the emulator and attach it to the object.
        :param training_dir:
            Directory where training data from trainginData is stored.
        :param fixed_params:
            Parameters to hold fixed. Only available if data in training_dir is a full hypercube, not a latin hypercube.
        :return: None
        """
        if type(training_dir) is not list:
            training_dir = [training_dir]

        xs, ys, yerrs = [], [], []
        for td in training_dir:
            x, y, yerr = self.get_data(td, {}, self.fixed_params, self.independent_variable)

            xs.append(x)
            ys.append(y)
            yerrs.append(yerr)

        x = np.vstack(xs)
        # hstack for 1-D
        y = np.hstack(ys)
        yerr = np.hstack(yerrs)
        y_hat = np.zeros(y.shape[1]) if len(y.shape) > 1 else 0  # self.y.mean(axis = 0)
        y -= y_hat

        #now, parition the data as specified by the user
        #note that ppe does not include overlap
        points_per_expert = int(1.0*x.shape[0]*self.overlap/self.experts)

        self.x = np.zeros((self.experts,points_per_expert , x.shape[1]))
        self.y = np.zeros((self.experts, points_per_expert))
        self.yerr  = np.zeros_like(self.y)

        if self.partition_scheme == 'random':
            shuffled_idxs = range(y.shape[0])
            np.random.shuffle(shuffled_idxs)
            
            #select potentially self.overlapping subets of the data for each expert
            for i in xrange(self.experts):
                self.x[i,:,:] = np.roll(x[shuffled_idxs, :], i*points_per_expert/self.overlap, 0)[:points_per_expert, :]
                self.y[i,:] = np.roll(y[shuffled_idxs], i*points_per_expert/self.overlap, 0)[:points_per_expert]
                self.yerr[i,:] = np.roll(yerr[shuffled_idxs], i*points_per_expert/self.overlap, 0)[:points_per_expert]

        else: #KDTree
            #TODO leaves won't all have the same size, must fix.
            kdtree = KDTree(x, leafsize = points_per_expert/self.overlap)
            leaves = get_leaves(kdtree)

            prev_idx, curr_idx = 0,0
            # If points cannot be evenly divided, there'll be some skipped ones.
            # We'll add them in at the end.
            n_missed = np.sum([len(leaf)%self.experts for leaf in leaves])
            missed_points = np.zeros(n_missed)
            missed_idx = 0

            for i, leaf in enumerate(leaves):
                shuffled_idxs = range(leaf.shape[0])
                np.random.shuffle(shuffled_idxs)

                leaf_ppe = int(1.0*self.overlap*leaf.shape[0]/self.experts)
                curr_idx = prev_idx+leaf_ppe

                #select potentially overlapping subets of the data for each expert
                for j in xrange(self.experts):

                    self.x[j,prev_idx:curr_idx,:] =\
                            np.roll(x[leaf[shuffled_idxs], :], j*leaf_ppe/self.overlap, 0)[:leaf_ppe, :]
                    self.y[j,prev_idx:curr_idx] = \
                            np.roll(y[leaf[shuffled_idxs]], j*leaf_ppe/self.overlap, 0)[:leaf_ppe]
                    self.yerr[j,prev_idx:curr_idx]\
                            = np.roll(yerr[leaf[shuffled_idxs]], j*leaf_ppe/self.overlap, 0)[:leaf_ppe]

                prev_idx = curr_idx
                nm = leaf.shape[0] % self.experts
                missed_points[missed_idx:missed_idx+nm] = leaf[shuffled_idxs][-nm:]
                missed_idx+=nm

            #now, distribute leftover points over experts
            missed_ppe = int(1.0*n_missed*self.overlap/self.experts)
            for i in xrange(self.experts):
                self.x[i, prev_idx:, :] = \
                    np.roll(x[missed_points, :], i * missed_ppe / self.overlap, 0)[:missed_ppe, :]
                self.y[j, prev_idx:] = \
                    np.roll(y[missed_points], i * missed_ppe / self.overlap, 0)[:missed_ppe]
                self.yerr[j, prev_idx:] \
                    = np.roll(yerr[missed_points], i * missed_ppe / self.overlap, 0)[:missed_ppe]

        ndim = x.shape[1]
        self.fixed_ndim = len(self.fixed_params)
        self.emulator_ndim = ndim  # The number of params for the emulator is different than those in sampling.

    def _build_gp(self, hyperparams):
        """
        Initialize the GP emulator using an MOE model.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        if 'metric' in hyperparams:
            metric = hyperparams['metric']
            del hyperparams['metric']
        else:
            metric = {}
        kernel = self._make_kernel(metric)
        # TODO is it confusing for this to have the same name as the sklearn object with a different API?
        # maybe it should be a property? or private?
        self.emulators = []

        for _x, _yerr in izip(self.x, self.yerr):
            emulator = george.GP(kernel)

            emulator.compute(_x, _yerr, sort=False, **hyperparams)  # NOTE I'm using a modified version of george!
            self.emulators.append(emulator)

    def _build_skl(self, hyperparams):
        """
        Build a scikit learn emulator using a mixtrue of experts.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        skl_methods = {'gbdt': GradientBoostingRegressor, 'rf': RandomForestRegressor, \
                       'svr': SVR, 'krr': KernelRidge}

        # Same kernel concerns as above.
        if self.method in {'svr', 'krr'}:  # kernel based method
            metric = hyperparams['metric'] if 'metric' in hyperparams else {}
            kernel = self._make_kernel(metric)
            if 'metric' in hyperparams:
                del hyperparams['metric']
            if self.method == 'svr':  # slight difference in these, sadly
                hyperparams['kernel'] = kernel.value
            else:  # krr
                hyperparams['kernel'] = lambda x1, x2: kernel.value(np.array([x1]), np.array([x2]))

        self.emulators = [skl_methods[self.method](**hyperparams) for i in xrange(self.experts)]

        for i, emulator, _x, _y in enumerate(izip(self.emulators, self.x, self.y)):
            emulator.fit(_x, _y)

    def _emulate_helper(self, t, gp_errs=False):
        """
        Helper function that takes a dependent variable matrix and makes a prediction.
        :param t:
            Dependent variable matrix. Assumed to be in the order defined by ordered_params
        :param gp_errs:
            Whether or not to return errors in the gp case
        :return:
            mu, err (if gp_errs True). Predicted value for dependetn variable t.
            mu and err both have shape (npoints*self.redshift_bin_centers*self.scale_bin_centers)
        """
        mu = np.zeros((self.experts, t.shape[0]))  # experts down, t deep
        err = np.zeros_like(mu)

        for i, (emulator, _y) in enumerate(izip(self.emulators, self.y)):
            if self.method == 'gp':
                local_mu, local_cov = emulator.predict(_y, t, mean_only= False)
                local_err = np.sqrt(np.diag(local_cov))
            else:
                local_mu = emulator.predict(t)
                local_err = 1.0 #weight with this instead of the errors.

            mu[i,:] = local_mu
            err[i,:] = local_err

        print mu.mean(axis = 0)
        print mu.std(axis = 0)

        #now, combine with weighted average
        combined_var = np.reciprocal(np.sum(np.reciprocal(err**2), axis = 0))
        combined_mu = combined_var*np.sum(np.reciprocal(err**2)*mu, axis = 0)

        # Reshape to be consistent with my otehr implementation
        if not gp_errs:
            return combined_mu
        return combined_mu, np.sqrt(combined_var)

    # TODO could make this learn the metric for other kernel based emulators...
    def train_metric(self, **kwargs):
        """
        Train the emulator. Has a spotty record of working. Better luck may be had with the NAMEME code.
        :param kwargs:
            Kwargs that will be passed into the scipy.optimize.minimize
        :return: success: True if the training was successful.
        """

        assert self.method == 'gp'

        # emulators is a list containing refernces to the same object. this should still work!

        # move these outside? hm.
        def nll(p):
            # Update the kernel parameters and compute the likelihood.
            # params are log(a) and log(m)
            ll = 0
            for emulator,_y in izip(self.emulators, self.y):
                emulator.kernel[:] = p
                ll += emulator.lnlikelihood(_y, quiet=True)

            # The scipy optimizer doesn't play well with infinities.
            return -ll if np.isfinite(ll) else 1e25

        # And the gradient of the objective function.
        def grad_nll(p):
            # Update the kernel parameters and compute the likelihood.
            gll = 0
            for emulator, _y in izip(self.emulators, self.y):
                emulator.kernel[:] = p
                gll += emulator.grad_lnlikelihood(_y, quiet=True)
            return -gll

        p0 = self.emulators[0].kernel.vector
        results = op.minimize(nll, p0, jac=grad_nll, **kwargs)
        # results = op.minimize(nll, p0, jac=grad_nll, method='TNC', bounds =\
        #   [(np.log(0.01), np.log(10)) for i in xrange(ndim+1)],options={'maxiter':50})

        for emulator in self.emulators:
            emulator.kernel[:] = results.x
            emulator.recompute()

        return results.success
