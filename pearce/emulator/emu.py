#!/bin/bash
"""The Emu object esentially wraps the George gaussian process code. It handles building, training, and predicting."""

import warnings
from glob import glob
from itertools import izip, product
from collections import OrderedDict
from os import path
from time import time
from abc import ABCMeta, abstractmethod

import numpy as np
import h5py
import george
from george.kernels import *
import scipy.optimize as op
from scipy.interpolate import interp1d
from scipy.linalg import inv, block_diag
from scipy.spatial import KDTree
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression


class Emu(object):
    '''Main Emulator base class. Cannot itself be instatiated; can only be accessed via subclasses.
       controls all loading, manipulation, and emulation of data.
    '''

    __metaclass__ = ABCMeta
    valid_methods = {'gp', 'svr', 'gbdt', 'rf', 'krr',
                     'linear'}  # could add more, coud even check if they exist in sklearn
    skl_methods = {'gbdt': GradientBoostingRegressor, 'rf': RandomForestRegressor, \
                   'svr': SVR, 'krr': KernelRidge, 'linear': LinearRegression}

    def __init__(self, filename, method='gp', hyperparams={}, fixed_params={}, independent_variable=None):
        '''
        Initialize the Emu
        :param filename:
            A .hdf5 file containing the training data, in the format of those generated by trainer.py
        :param method:
            Emulation method. Valid methods are 'gp', 'svr', 'gbdt', 'rf', and 'krr'. Default is 'gp'. GP is
            conducted by george, all others are executed by sklearn. Kernel based sklearn methods use a george kernel.
        :param hyperparams:
            Hyperparameters for the emulator. Gp hyperparams are passed into george, others are passed into
            sklearn. A special hyperparam is 'metric', which determines the metric for kernel-based methods.
            See documentation for a full list of hyperparameters.
             Default is {}.
        :param fixed_params:
            Parameters to hold fixed during training. Key is the name of the param, value if the value to hold fixed.
            The only parameters that can be fixed are 'cosmo', 'HOD', 'r', and 'z'. 'r' and 'z' can be fixed to floats
            of scale (distance in Mpc, angle in degrees, etc) and redshift respectively.
            Cosmo and HOD can only be fixed to an integer number, representing the index of the cosmo/HOD to hold fixed
            across HODs/Cosmologies respectively. Multiple fixed params can be specified.
             Default is {}.
        :param independent_variable:
            Indepent variable to emulate. Default is None, which just emulates the iv in the training data
            directly. Presently the only acceptable option is 'r2', which emulates r^2 times the
            parameter in the training data.
        '''

        assert method in self.valid_methods

        assert independent_variable in {None, 'r2'}  # no bias for now.

        self.method = method

        self.fixed_params = fixed_params
        self.independent_variable = independent_variable

        self.load_training_data(filename)
        self.build_emulator(hyperparams)

    ###Data Loading and Manipulation####################################################################################
    # This function is a little long, but I'm not certain there's a need to break it up
    # it's shorter than it used to be, too.
    def get_data(self, filename, fixed_params, independent_variable, attach_params = False):
        """
        Read data in the format compatible with this object and return it.

        :param filename:
            A .hdf5 file containing the training data, in the format of those generated by trainer.py
        :param fixed_params:
            Parameters to hold fixed during training. Key is the name of the param, value if the value to hold fixed.
            The only parameters that can be fixed are 'cosmo', 'HOD', 'r', and 'z'. 'r' and 'z' can be fixed to floats
            of scale (distance in Mpc, angle in degrees, etc) and redshift respectively.
            Cosmo and HOD can only be fixed to an integer number, representing the index of the cosmo/HOD to hold fixed
            across HODs/Cosmologies respectively. Multiple fixed params can be specified.
        :param independent_variable:
            Independant variable to emulate. Options are xi, r2xi
        :return: x, y, yerr, ycov, all numpy arrays.
                 x is (n_data_points, n_params)
                 y is (n_data_points, ), yerr is (n_data_points)
                 and ycov (n_data_points, n_scale_bins, n_scale_bins), a "list" of covaraince matrices,
                 to do with what you will
        """
        assert path.isfile(filename)
        # fixed params can only fix an hod index, cosmo index, or z or r
        assert len(fixed_params) <= 4
        assert all(key in {'cosmo', 'HOD', 'z', 'r'} for key in fixed_params)

        for key in ['cosmo', 'HOD']:
            if key in fixed_params:
                assert type(fixed_params[key]) is int

        f = h5py.File(filename, 'r')

        # get global attributes from the file
        cosmo_param_names = f.attrs['cosmo_param_names']
        hod_param_names = f.attrs['hod_param_names']

        cosmo_param_vals = f.attrs['cosmo_param_vals']
        hod_param_vals = f.attrs['hod_param_vals']

        scale_factors = f.attrs['scale_factors']
        redshift_bin_centers = 1.0/scale_factors - 1 # emulator works in z, sims in a.
        if 'z' in fixed_params:
            assert fixed_params['z'] in redshift_bin_centers

        scale_bins = f.attrs['scale_bins']
        scale_bin_centers = (scale_bins[1:] + scale_bins[:-1])/2.0 if scale_bins is not None else None
        if 'r' in fixed_params:
            # this wil also fale if scb is None. but you can't fix when it's none anyway so.
            # may wanna have a friendlier error message htough.
            assert np.any(np.abs(fixed_params['r'] - scale_bin_centers) < 1e-4) # may need to include a fudge factor here
            r_idx = np.argmin(np.abs(fixed_params['r'] -  scale_bin_centers))

        # construct ordered_params
        # ordered_params is an ordered dict whose keys are the parameters in the
        # order they are in in the data. The values are their bounds in the training data
        if 'HOD' in fixed_params and 'cosmo' not in fixed_params:
            # Why not?
            #if 'cosmo' in fixed_params:
            #    raise ValueError("Can't fix both HOD and cosmology!")
            min_max_vals = zip(cosmo_param_vals.min(axis=0), cosmo_param_vals.max(axis=0))
            ordered_params = OrderedDict(izip(cosmo_param_names, min_max_vals))
        elif 'cosmo' in fixed_params and 'HOD' not in fixed_params:
            min_max_vals = zip(hod_param_vals.min(axis=0), hod_param_vals.max(axis=0))
            ordered_params = OrderedDict(izip(hod_param_names, min_max_vals))
        elif 'cosmo' not in fixed_params and 'HOD' not in fixed_params:
            op_names = list(cosmo_param_names[:])
            op_names.extend(hod_param_names)

            min_max_vals = zip(np.r_[cosmo_param_vals.min(axis=0), hod_param_vals.min(axis=0)], \
                               np.r_[cosmo_param_vals.max(axis=0), hod_param_vals.max(axis=0)])
            ordered_params = OrderedDict(izip(op_names, min_max_vals))
        else:
            ordered_params = OrderedDict()

        # NOTE if its single_valued, may have to fudge this somehow?
        if 'z' not in fixed_params:
            ordered_params['z'] = (np.min(redshift_bin_centers), np.max(redshift_bin_centers))

        if 'r' not in fixed_params:
            ordered_params['r'] = (np.log10(np.min(scale_bins)), np.log10(np.max(scale_bins)))

        if attach_params: #attach certain parameters to the object
            self.obs = f.attrs['obs']
            self.redshift_bin_centers = redshift_bin_centers
            self.scale_bin_centers = scale_bin_centers
            self.n_bins = len(scale_bin_centers) if scale_bin_centers is not None else 1
            self._ordered_params = ordered_params

        # append files to a list, then concatenate at the end
        x = []
        y = []
        ycov = []

        # book keeping vars.
        # only want to warn the user once.
        give_warning = False
        # these can be useful for debugging
        num_skipped = 0
        num_used = 0

        for cosmo_group_name, cosmo_group in f.iteritems():
            # we're fixed to a particular cosmology #
            cosmo_no = int(cosmo_group_name[-2:])
            if 'cosmo' in fixed_params and cosmo_no != fixed_params['cosmo']:
                    continue
            for sf_group_name, sf_group in cosmo_group.iteritems():
                # likewise
                z = 1.0/float(sf_group_name[-5:]) - 1.0

                # TODO fudge factors?
                if 'z' in fixed_params and z != fixed_params['z']:
                    continue

                obs_dset = sf_group['obs']
                cov_dset = sf_group['cov']

                cosmo = cosmo_param_vals[cosmo_no, :]
                # efficiency note. If I don't iterate over the datasets, just the indicies,
                # I avoid loading them from disk in fixed HOD scenarios
                # Those will be rare enough for now that I don't care.
                for HOD_no, (_obs, _cov) in enumerate(izip(obs_dset, cov_dset)):

                    if "HOD" in fixed_params and HOD_no != fixed_params['HOD']:
                        continue
                    if any(np.any(np.isnan(arr)) for arr in [_obs, _cov]):
                        # skip NaN points. May wanna change this behavior.
                        give_warning = True
                        num_skipped += 1
                        continue

                    HOD = hod_param_vals[HOD_no, :]

                    params = []
                    # I wonder if this is annoyingly inefficient. Probably not a huge deal.
                    if 'cosmo' not in fixed_params:
                        params.extend(list(cosmo))
                    if 'HOD' not in fixed_params:
                        params.extend(list(HOD))
                    if 'z' not in fixed_params:
                        params.append(z)
                    # handle fixed r differently than the others

                    if 'r' in fixed_params:
                        params = np.array(params)

                        x.append(params)

                        #we hve to transform the data (take a log, multiply, etc)
                        # TODO this may not work with things like r2 anymore
                        _o, _c = self._iv_transform(independent_variable, _obs, _cov)
                        y.append(np.log10(np.array([_o[r_idx]])))
                        ycov.append(np.array(_c[r_idx, r_idx]))

                    else:
                        _params = np.zeros((scale_bin_centers.shape[0], len(params) + 1))
                        _params[:, :-1] = params
                        _params[:, -1] = np.log10(scale_bin_centers)
                        x.append(_params)

                        _o, _c = self._iv_transform(independent_variable, _obs, _cov)
                        y.append(_o)
                        ycov.append(_c)

                    num_used += 1

        if give_warning:
            warnings.warn('WARNING: NaN detected. Skipped %d points in training data.' % (num_skipped))

        f.close()
        # stack so xs have shape (n points, n params)
        # ys have shape (npoints)
        # and ycov has shape (n_bins, n_bins, n_points/n_bins)
        return np.vstack(x), np.hstack(y), np.dstack(ycov)


    def load_training_data(self, filename):
        """
        Read the training data for the emulator and attach it to the object.

        :param filename:
            A .hdf5 file conatining training data in the format generated by trainer.py
        :return: None
        """

        # make sure we attach metadata to the object
        x, y, ycov = self.get_data(filename, self.fixed_params, self.independent_variable, attach_params=True)

        self.x = x
        self.y = y

        # in general, the full cov matrix will be too big, and we won't need it. store the diagonal, and
        # an average
        split_ycov = np.dsplit(ycov, ycov.shape[-1])
        #fullcov = block_diag(*[yc[:,:,0] for yc in split_ycov])
        self.yerr = np.sqrt(np.hstack(np.diag(syc[:,:,0]) for syc in split_ycov))
        #self.yerr = np.hstack([yerr for i in xrange(self.x.shape[0] / fullcov.shape[0])])


        #compute the average covaraince matrix
        self.ycov = np.mean(split_ycov, axis = 2)

        # subtract off the mean; this should lead to better performance for the GP
        self.y_hat = self.y.mean(axis=0)
        self.y -= self.y_hat

        ndim = self.x.shape[1]
        self.emulator_ndim = ndim  # The number of params for the emulator is different than those in sampling.

    def get_param_names(self):
        """
        Helper function that returns the names of the parameters in the emulator.
        :return: names, a list of parameter names (in order)
        """
        return self._ordered_params.keys()

    def get_param_bounds(self, param):
        """
        Return the emulator bounds for parameter param. Raises a ValueError if it can't be found.
        :param param:
            String, the name of the parameter to return bounds for.
        :return: bounds, a 2 element tuple with the lower and higher bounds of point param.
        """
        try:
            return self._ordered_params[param]
        except KeyError:
            raise KeyError("Parameter %s could not be found." % param)

    def _get_ordered_params(self, dirname, fixed_params, with_fixed_params=False):
        """
        Load the ordered params from directory 'dirname.' Remove params that
        are held fixed by the emulator. Check that there are no violations and,
        if it is not currently attached to the object, attach it. Optionally return
        a version that does not have the fixed parameters removed, which is necessary for some functions
        :param dirname:
            name of the directory to load ordered_params from
        :param fixed_params:
            Iterator of fixed parameters. Will be removed from ordered params
        :param with_fixed_params:
            Boolean. Whether to return the ordered params without the fixed_params removed.
            This is unfrotunately a little hacky, but it is necessary for the training_file_loc
            feature.
        :return: if with_fixed_params, ordered_params_with_fixed_params, a version of the attached object with the fixed
        params left in place.
        """
        # get the defined param ordered for the training data, and ensure its ok.
        ordered_params = params_file_reader(dirname)

        # this will not contain 'z' or 'r'. Add them to the end.
        # note that the bounds of these parameters will be updated later in get_data, if not here.
        if hasattr(self, "redshift_bin_centers") and hasattr(self, "scale_bin_centers"):
            # if we know the actual bounds, attach them
            ordered_params['z'] = (np.min(self.redshift_bin_centers), np.max(self.redshift_bin_centers))
            ordered_params['r'] = (np.min(self.scale_bin_centers), np.max(self.scale_bin_centers))
        else:
            ordered_params['z'] = (0, 1)
            ordered_params['r'] = (0, 1)

        # store in case we have to return this
        op_with_fixed = ordered_params.copy()

        for pname in fixed_params:
            if pname in ordered_params:
                del ordered_params[pname]
            else:
                raise KeyError('Parameter %s is in fixed_params but not in ordered_params!' % pname)

        attached_or = getattr(self, "_ordered_params", None)
        # attach the ordered params if its new. Otherwise, ensure that the ordering in this dir is the same as what
        # we have already.
        if attached_or is None:
            self._ordered_params = ordered_params
        else:
            try:
                assert ordered_params == attached_or
            except AssertionError:
                raise AssertionError("ordered_params in %s did not match the value attached to this object." % dirname)

        if with_fixed_params:
            return op_with_fixed

    # TODO Should I unify some syntax between this and the one below?
    def check_param_names(self, param_names, ignore=[]):
        """
        Checks that all parameter names in param_names are defined in the emulator, and vice versa.
        :param param_names:
            List of strings of parameter names to compare to _ordered_params
        :param ignore"
            A list of parameters to ignore. There are many use cases where parameters like 'r' are definedi n the
            emulator but not used in the emulator, so we wouldn't want params to define them.
        :return:
            True if all param_names are in ordered_params, and vice verse. False otherwise
        """
        op_set = set(self._ordered_params.iterkeys())
        for ig in ignore:
            op_set.remove(ig)

        ip_set = set(param_names)

        return len(op_set ^ ip_set) == 0

    def _check_params(self, params):
        """
        Assert that all keys in params are defined in ordered params, and vice versa. Raises an AssertionError otherwise.
        Will also raise a warning if trying to emulate out of bounds.
        :param params:
            Dictionary of params, where the key is the name and the value is the value it holds. Value can also
            be a numpy array.
        :return: None
        """
        try:
            assert self.check_param_names(params.keys())
        except AssertionError:
            output = "The input_params passed into get_data did not match those the Emu knows about. \
                                              It's possible fixed_params is missing a parameter, or you defined an extra one. \
                                              Additionally, orded_params could be wrong too!\n"

            op_set = set(self._ordered_params.iterkeys())
            ip_set = set(params.iterkeys())

            ip_not_op = ip_set - op_set
            op_not_ip = op_set - ip_set

            if ip_not_op:
                output += 'Param %s was in the input but not the training data.\n' % list(ip_not_op)[0]
            if op_not_ip:
                output += 'Param %s was in the training data but not the input.\n' % list(op_not_ip)[0]

            raise AssertionError(output)

        for pname, (plow, phigh) in self._ordered_params.iteritems():
            try:
                # check if they're in bounds, else raise an informative warning
                val = params[pname]
                # TODO wish i didn't have to hardcode this
                # NOTE insert from merge, not sure if bad
                if pname == 'r':
                    val = np.log10(val)

                assert np.all(plow <= val) and np.all(val <= phigh)
            except AssertionError:
                if type(val) is float:
                    warnings.warn("Emulator value for %s %.3f is outside the bounds (%.3f, %.3f) of the emulator." % (
                        pname, val, plow, phigh))
                else:
                    warnings.warn("One value for %s is outside the bounds (%.3f, %.3f) of the emulator." % (
                        pname, plow, phigh))

    def _iv_transform(self, independent_variable, obs, cov=None):
        """
        Independent variable tranform. Helper function that consolidates this operation all in one place.
        :param independent_variable:
            Which iv to transform to. Current options are None (just take log) and r2.
        :param obs:
            Observable to transform (xi, wprp, etc.)
        :param cov:
            Covariance of obs. Optional.
        :return:
            y, y_cov the transformed iv's for the emulator. If cov is none, only return y.
        """
        # NOTE this invalidates old training data
        if independent_variable is None or independent_variable == 'wt':
            # y = np.log10(obs)
            # Approximately true, may need to revisit
            # yerr[idx * NBINS:(idx + 1) * NBINS] = np.sqrt(np.diag(cov)) / (xi * np.log(10))
            # y_cov = cov/np.outer(obs * np.log(10), obs*np.log(10))
            y = obs
            y_cov = cov

        elif independent_variable == 'r2':  # r2xi
            y = obs * self.scale_bin_centers * self.scale_bin_centers
            y_cov = cov * np.outer(self.scale_bin_centers, self.scale_bin_centers)
        else:
            raise ValueError('Invalid independent variable %s' % independent_variable)

        # if independent_variable == 'bias':
        #    y[idx * NBINS:(idx + 1) * NBINS] = xi / xi_mm
        #    ycovs.append(cov / np.outer(xi_mm, xi_mm))
        if cov is not None:
            return y, y_cov
        return y

    def _sort_params(self, t, argsort=False):
        """
        Sort the parameters in a defined away given the orderering.
        :param t:
            Parameter vector to sort. Should have dims (N, N_params) and be in the order
            defined by ordered_params
        :param argsort:
            If true, return indicies that would sort the array rather than the sorted array itself.
            Default is False.
        :return:
            If not argsort, returns the sorted array by column and row. 
            If argsort, return the indicies that would sort the array.
        """
        if t.shape[0] == 1:
            if argsort:
                return np.array([0])
            return t  # a single row array is already sorted!

        # the sorting procedure is black magic. Don't touch unless you're smarter than me.
        if argsort:  # returns indicies that would sort the array
            # weird try structure because this view is very tempermental!
            try:
                idxs = np.argsort(t.view(','.join(['float64' for _ in xrange(min(t.shape))])),
                                  order=['f%d' % i for i in xrange(min(t.shape))], axis=0)
            except ValueError:  # sort with other side
                idxs = np.argsort(t.view(','.join(['float64' for _ in xrange(max(t.shape))])),
                                  order=['f%d' % i for i in xrange(max(t.shape))], axis=0)

            return idxs[:, 0]

        try:
            t = np.sort(t.view(','.join(['float64' for _ in xrange(min(t.shape))])),
                        order=['f%d' % i for i in xrange(min(t.shape))], axis=0).view(np.float)
        except ValueError:  # sort with other side
            t = np.sort(t.view(','.join(['float64' for _ in xrange(max(t.shape))])),
                        order=['f%d' % i for i in xrange(max(t.shape))], axis=0).view(np.float)

        return t

    ###Emulator Building and Training###################################################################################

    def build_emulator(self, hyperparams):
        """
        Initialization of the emulator from recovered training data. Calls submethods depending on "self.method"
        :param hyperparams
            A dictionary of hyperparameter kwargs for the emulator
        :return: None
        """

        if self.method == 'gp':
            self._build_gp(hyperparams)
        else:  # an sklearn method
            self._build_skl(hyperparams)

    @abstractmethod
    def _build_gp(self, hyperparams):
        pass

    @abstractmethod
    def _build_skl(self, hyperparams):
        pass

    def _get_initial_guess(self, independent_variable):
        """
        Return the initial guess of the metric for the emulator, based on what the iv is. Guesses are learned from
        previous experiments.
        :param independent_variable:
            Which variable to return the guesses for.
        :return: initial_guesses, a dictionary of the guess for each parameter
        """

        # default
        ig = {'amp': 1}
        ig.update({pname: 0.1 for pname in self._ordered_params})

        if self.obs == 'xi':
            if independent_variable is None:
                ig.update({'amp': 0.481, 'logMmin': 0.1349, 'sigma_logM': 0.089,
                           'logM0': 2.0, 'logM1': 0.204, 'alpha': 0.039,
                           'f_c': 0.041, 'r': 0.040, 'z': 1.0})
            else:
                # could have other guesses for this case, but don't have any now
                # leave this structure in case I make more later
                pass
        elif self.obs == 'wp':
            # TODO parameter name has changed, update
            if independent_variable is None:

                ig.update({'ombh2': 7.41265e-5, 'omch2': 5.04824e1, 'w0': 1.301436e-2,
                            'ns': 3.89150886e-4, 'ln10As': 9.83101864, 'H0': 1.5611286e5,
                            'Neff': 9.71398614e2, 'logM1': 1.148312e-2, 'logMmin': 1.6566059e1,
                            'f_c': 1.4426e1 ,'logM0': 6.2487e1, 'sigma_logM': 2.32469,
                            'alpha': 4.5149677,'r': 1.94217e-2,
                           'amp': 2.9004304, 'z': 1.0,
                           'mean_occupation_satellites_assembias_split1': 21.02835102,
                           'mean_occupation_satellites_assembias_slope1': 225.64738711,
                           'mean_occupation_satellites_assembias_param1': 89.17850468,
                           'mean_occupation_satellites_assembias_corr1': 1.15892e3,
                           'mean_occupation_centrals_assembias_split1': 66.97808312,
                           'mean_occupation_centrals_assembias_slope1': 179.90950523,
                           'mean_occupation_centrals_assembias_param1': 80.75541473,
                           'mean_occupation_centrals_assembias_corr1': 3.9659e6})

        elif self.obs == 'wt':
            # TODO parameter name has changed, update
            if independent_variable is None:
                ig.update({'logMmin': 3.84345171761, 'f_c': 1.80188170556, 'logM0':14.11665461,
                    'sigma_logM': 17.7239294539, 'alpha': 0.18960822912, 'r': 0.306139450843,
                    'logM1': 1.0554692015, 'amp': 2.52461085754, 'z': 1.0,
                    'mean_occupation_satellites_assembias_param1':2.45298658504,
                    'mean_occupation_centrals_assembias_param1':27.2832783025})

        elif self.obs == 'ds':
            if independent_variable is None:
                ig.update({'logMmin': 3.6176310032, 'f_c': 0.574766597478, 'logM0':45.5862423685,
                    'sigma_logM': 1.3220351031, 'alpha': 2.31333123015, 'r': 0.17221523,
                    'logM1': 2.16453187021, 'amp': 0.42810696, 'z': 1.0,
                    'mean_occupation_satellites_assembias_param1': 6.65437027486,
                    'mean_occupation_centrals_assembias_param1': 3.1539620393})


        else:
            pass  # no other guesses saved yet.

        # remove entries for variables that are being held fixed.
        for key in self.fixed_params.iterkeys():
            if key in ig:
                del ig[key]

        return ig

    def _make_kernel(self, metric={}):
        """
        Helper method to build a george kernel for GP's and kernel-based regressions.
        :param metric:
            Hyperparams for kernel determining relative length scales and amplitudes. Default is empty dict.
            In that case, the initial guesses from the object will be used.
        :return:
            A george ExpSquredKernel object with this metric
        """

        if not metric:
            ig = self._get_initial_guess(self.independent_variable)
        else:
            ig = metric  # use the user's initial guesses

        metric = [ig['amp']]
        for pname in self._ordered_params:
            try:
                metric.append(ig[pname])
            except KeyError:
                raise KeyError('Key %s was not in the metric.' % pname)
                #metric.append(1.0)

        metric = np.array(metric)


        a = metric[0]
        # TODO other kernels?
        return a * ExpSquaredKernel(metric[1:], ndim=self.emulator_ndim)
        # return a * Matern32Kernel(metric[1:], ndim=self.emulator_ndim)

    ###Emulation and methods that Utilize it############################################################################
    def emulate(self, em_params, gp_errs=False):
        """
        Perform predictions with the emulator.
        :param em_params:
            Dictionary of what values to predict at for each param. Values can be
            an array or a float.
        :param gp_errs:
            Boolean, decide whether or not to return the errors from the gp prediction. Default is False.
            Will throw error if method is not gp.
        :return: mu, (errs)
                  The predicted value and the uncertainties for the predictions
                  mu and errs both have shape (npoints,)
        """

        # only has meaning for gp's
        assert not gp_errs or self.method == 'gp'

        input_params = {}
        # input_params.update(self.fixed_params)
        input_params.update(em_params)

        self._check_params(input_params)

        # create the dependent variable matrix
        t_list = [input_params[pname] for pname in self._ordered_params if pname in em_params]
        t_grid = np.meshgrid(*t_list)
        t = np.stack(t_grid).T
        t = t.reshape((-1, self.emulator_ndim))

        # TODO george can sort?
        _t = self._sort_params(t)
        if _t.shape == t.shape:  # protect against weird edge case...
            t = _t

        if len(t.shape) == 1:
            t = np.array([t])

        return self._emulate_helper(t, gp_errs)

    @abstractmethod
    def _emulate_helper(self, t, gp_errs=False):
        pass

    def emulate_wrt_r(self, em_params, r_bin_centers, gp_errs=False):
        """
        Helper function to emulate over r bins.
        :param em_params:
            Parameters to predict at
        :param r_bin_centers:
            Radial bins to predict at
        :param gp_err:
            Boolean, whether or not to use the errors from the GP. Default is False.
            If method is not 'gp', will throw an error
        :return: mu, (errs)
                mu has shape (n_points, len(r_bin_centers))
                errs, if returned, has the same shape
        """
        # TODO this should have scale bin centers as teh default, duh!
        ep = {}
        ep.update(em_params)
        # extract z from the emulator params, if it's there
        # we will use this to call emulate_wrt_r_z below
        z_bin_centers = np.array([])
        if 'z' in ep:
            z_bin_centers = ep['z']
            if type(z_bin_centers) is float:
                z_bin_centers = np.array([z_bin_centers])
            del ep['z']
        elif 'z' not in self.fixed_params:
            raise ValueError("Please specify z in emulate_wrt_z")
        out = self.emulate_wrt_r_z(ep, r_bin_centers, z_bin_centers, gp_errs)

        # Extract depending on if there are errors
        if gp_errs:
            _mu, _errs = out
        else:
            _mu = out

        # now, reshape to have shape (-1, len(r_bin_centers))
        mu = _mu.reshape((-1, r_bin_centers.shape[0]))
        if not gp_errs:
            return mu

        errs = _errs.reshape(mu.shape)
        return mu, errs

    def emulate_wrt_z(self, em_params, z_bin_centers, gp_errs=False):
        """
        Helper function to emulate over z bins.
        :param em_params:
            Parameters to predict at
        :param z_bin_centers:
            Redshift bins to predict at
        :param gp_err:
            Boolean, whether or not to use the errors from the GP. Default is False.
            If method is not 'gp', will throw an error
        :return:mu, (errs)
                mu has shape (n_points, len(z_bin_centers))
                errs, if returned, has the same shape
        """
        ep = {}
        ep.update(em_params)
        r_bin_centers = np.array([])
        if 'r' in ep:
            r_bin_centers = ep['r']
            if type(r_bin_centers) is float:
                r_bin_centers = np.array([r_bin_centers])
            del ep['r']

        out = self.emulate_wrt_r_z(ep, r_bin_centers, z_bin_centers, gp_errs)
        # extract errors if they're returned
        if gp_errs:
            _mu, _errs = out
        else:
            _mu = out

        # now, reshape to have shape (-1, len(z_bin_centers))
        # The swapaxes are necessary to make sure the reshape works properly
        mu = _mu.swapaxes(1, 2).reshape((-1, z_bin_centers.shape[0]))

        if not gp_errs:
            return mu
        errs = _errs.swapaxes(1, 2).reshape(mu.shape)
        return mu, errs

    def emulate_wrt_r_z(self, em_params, r_bin_centers, z_bin_centers, gp_errs=False):
        """
        Conveniance function. Add's 'r' and 'z' to the emulation automatically, as this is the
        most common use case.
        :param em_params:
            Dictionary of what values to predict at for each param. Values can be array
            or float.
        :param r_bin_centers:
            numpy array. Centers of scale bins to predict at, for each point in HOD-space.
            Note this function takes their real-space values, not log-space.
        :param z_bin_centers:
            numpy array. Centers of redshift bins to predict at, for each point in HOD-space
        :param gp_errs:
            Wheter to return the errors of the Gaussian process or not.
        :return: mu, errs (if gp_errs == True)
                both have been reshaped to have shape (-1, len(z_bin_centers), len(r_bin_centers))
        """
        vep = dict(em_params)
        # take the log of r_bin_centers
        rpc = np.log10(r_bin_centers) if np.any(r_bin_centers) else np.array([])  # make sure not to throw an error

        # now, put them into the emulation dictionary.
        for key, val in izip(['r', 'z'], (rpc, z_bin_centers)):
            if key not in self.fixed_params and val.size:  # not fixed and the array is nonzero
                if key not in vep:
                    vep[key] = val
                else:
                    raise ValueError("The parameter %s has been specified twice in emulate_wrt_r_z!" % key)

        # now, emulate.
        out = self.emulate(vep, gp_errs)
        if gp_errs:
            _mu, _errs = out
        else:
            _mu = out

        # account for weird binning isues.
        if not z_bin_centers.shape[0]:
            if not r_bin_centers.shape[0]:
                mu = _mu.reshape((-1, 1, 1))
            else:
                mu = _mu.reshape((-1, 1, r_bin_centers.shape[0]))
        elif not r_bin_centers.shape[0]:
            mu = _mu.reshape((-1, z_bin_centers.shape[0], 1))
        else:
            mu = _mu.reshape((-1, z_bin_centers.shape[0], r_bin_centers.shape[0]))

        if not gp_errs:
            return mu
        errs = _errs.reshape(mu.shape)
        return mu, errs

    # TODO Jeremey keeps konwn uncertainties, I should do the same here, or near to here.
    def estimate_uncertainty(self, truth_dir, N=None):
        """
        Estimate the uncertainty of the emulator by comparing to a "test" box of true values.
        :param truth_dir:
            Name of a directory of true test values, of the same format as the train_dir
        :param N:
            Number of points to compare to. If None (default) will use all points. Else will select random sample.
        :return:
            covariance matrix with dim n_binsxn_bins. Will only have diagonal elemtns of est. uncertainties.
        """
        rms_err = self.goodness_of_fit(truth_dir, N, statistic='rms')

        return np.diag(rms_err ** 2)

    def hyperparam_random_grid_search(self, N, param_dist=None):
        """
        """
        # TODO docs
        if self.method == 'gp':
            return self._kernel_random_grid_search(N, param_dist)
        else:
            #return self._skl_random_grid_search(N, param_dist)
            raise NotImplementedError

    def _kernel_random_grid_search(self,N, param_dist):
        """
        """
        # TODO docs
        from sys import stdout
        param_names = self.get_param_names()
        param_names.append('amp') #dont forget!
        if param_dist is None: #default, do every parameter over a large range
            param_ranges = [(-6, 6) for p in param_names]
            param_dist = OrderedDict(zip(param_names, param_ranges))
            fixed_param_names = []
        else:
            # not all the parameters, hold some fixed? ugh.
            if type(param_dist) is not OrderedDict:
                param_dist = OrderedDict(param_dist)
            param_names = param_dist.keys()
            assert all(up in set(param_names) for up in user_param_names)
            fixed_param_names = list(set(param_names) - set(user_param_names))


        np.random.seed(int(time()))

        points = []
                                                                                                                                            # by linspacing each parameter and shuffling, I ensure there is only one point in each row, in each dimension.
        # TODO  min max LHC?
        for plow, phigh in param_dist.itervalues():
            point = np.logspace(plow, phigh, num=N)
            np.random.shuffle(point)  # makes the cube random.
            points.append(point)

        LHC = np.stack(points).T

        ig = self._get_initial_guess(self.independent_variable)

        likelihoods = np.zeros((LHC.shape[0],))
        t0 = time()
        for i, point in enumerate(LHC):
            print i,point
            metric = dict(zip(param_names,point))

            for fixed_param in fixed_param_names:
                metric[fixed_param] = ig[fixed_param]
            self._build_gp({'metric': metric})
            ll, _ = self._emulator_lnlikelihood()
            print time()-t0
            stdout.flush()
            likelihoods[i] = ll

        self._build_gp({}) #rebuild the default

        sorted_idxs = np.argsort(likelihoods)[::-1]
        return likelihoods[sorted_idxs], LHC[sorted_idxs]


    # only predicts wrt r. don't know if that's an ihmssue.
    def goodness_of_fit(self, truth_dir, N=None, statistic='r2'):
        """
        Calculate the goodness of fit of an emulator as compared to some validation data.
        :param truth_dir:
            Directory structured similarly to the training data, but NOT used for training.
        :param N:
            Number of points to use to calculate G.O.F. measures. "None" tests against all values in truth_dir. If N
            is less than the number of points, N are randomly selected. Default is None.
        :param statistic:
            What G.O.F. statistic to calculate. Default is r2. Other options are rmsfd, abs(olute), and rel(ative).
        :return: values, a numpy arrray of the calculated statistics at each of the N training points.
        """
        assert statistic in {'r2', 'rms', 'rmsfd', 'abs', 'log_abs', 'frac', 'log_frac'}
        if N is not None:
            assert N > 0 and int(N) == N

        sub_dirs, _ = self._get_z_subdirs(truth_dir, fixed_zs=self.fixed_params.get('z', None))

        x, y, _, _ = self.get_data(truth_dir, {}, self.fixed_params, self.independent_variable)

        bins, _, _, _, _ = global_file_reader(sub_dirs[0])
        bin_centers = (bins[1:] + bins[:-1]) / 2
        scale_nbins = len(bin_centers)

        y = y.reshape((-1, scale_nbins))

        np.random.seed(int(time()))

        if N is not None:  # make a random choice
            idxs = np.random.choice(x.shape[0], N, replace=False)

            x, y = x[idxs], y[idxs]
        pred_y = self._emulate_helper(x, False)
        pred_y = pred_y.reshape((-1, scale_nbins))

        # TODO untested
        if np.any(bin_centers != self.scale_bin_centers):
            bin_centers = bin_centers[self.scale_bin_centers[0] <= bin_centers <= self.scale_bin_centers[-1]]
            new_mu = []
            for mean in pred_y:
                xi_interpolator = interp1d(self.scale_bin_centers, mean, kind='slinear')
                interp_mean = xi_interpolator(bin_centers)
                new_mu.append(interp_mean)
            pred_y = np.array(new_mu)
            y = y[:, self.scale_bin_centers[0] <= bin_centers <= self.scale_bin_centers[-1]]

        if statistic == 'rmsfd':
            return np.sqrt(np.mean((((pred_y - y) ** 2) / (y ** 2)), axis=0))

        elif statistic == 'rms':
            return np.sqrt(np.mean(((pred_y - y) ** 2), axis=0))

        # TODO sklearn methods can do this themselves. But i've already tone the prediction!
        elif statistic == 'r2':  # r2
            SSR = np.sum((pred_y - y) ** 2, axis=0)
            SST = np.sum((y - y.mean(axis=0)) ** 2, axis=0)

            return 1 - SSR / SST

        elif statistic == 'abs':
            return 10 ** pred_y - 10 ** y
            # return np.mean(10 ** pred_y - 10 ** y, axis = 0)
        elif statistic == 'log_abs':
            return pred_y - y
            # return np.mean((pred_y - y), axis=0)
        elif statistic == 'log_frac':  # 'rel'
            return (pred_y - y) / y
            # return np.mean((pred_y - y) / y, axis=0)
        else:  # 'frac'
            return (10 ** pred_y - 10 ** y) / (10 ** y)

    @abstractmethod
    def _emulator_lnlikelihood(self):
        pass

    @abstractmethod
    def train_metric(self,p0, **kwargs):
        pass

    # TODO this feature is not super useful anymore, and also is poorly defined w.r.t non gp methods.
    # did a lot of work on it tho, maybe i'll leave it around...?
    # TODO this feature is no longer correct with EC
    def _loo_errors(self, y, t):
        """
        Calculate the LOO Jackknife error matrix. This is implemented using the analytic LOO procedure,
        which is much faster than re-doing an inversion for each sample. May be useful if the GP's matrix is not
        accurate.
        :param y:
            Values of the independent variable for the training points, used in the prediction.
        :param t:
            Values of the dependant variables to predict at.
        :return:
            jk_cov: a covariance matrix with the dimensions of cov.
        """
        # from time import time

        assert self.method == 'gp'

        if isinstance(self, ExtraCrispy):
            emulator = self._emulators[0]  # hack for EC, do somethign smarter later
        else:
            emulator = self._emulator

        # We need to perform one full inverse to start.
        K_inv_full = emulator.solver.apply_inverse(np.eye(emulator._alpha.size),
                                                   in_place=True)

        # TODO deepcopy?
        x = self.x[:]

        N = K_inv_full.shape[0]

        mus = np.zeros((N, t.shape[0]))
        # t0 = time()

        # iterate over training points to leave out
        for idx in xrange(N):
            # swap the values of the LOO point and the last point.
            x[[N - 1, idx]] = x[[idx, N - 1]]
            y[[N - 1, idx]] = y[[idx, N - 1]]

            K_inv_full[[idx, N - 1], :] = K_inv_full[[N - 1, idx], :]
            K_inv_full[:, [idx, N - 1]] = K_inv_full[:, [N - 1, idx]]

            # the inverse of the LOO GP
            # formula found via MATH
            K_m_idx_inv = K_inv_full[:N - 1, :][:, :N - 1] \
                          - np.outer(K_inv_full[N - 1, :N - 1], K_inv_full[:N - 1, N - 1]) / K_inv_full[N - 1, N - 1]

            alpha_m_idx = np.dot(K_m_idx_inv, y[:N - 1] - emulator.mean(x[:N - 1]))

            Kxxs_t = emulator.kernel.value(t, x[:N - 1])

            # Store the estimate for this LOO GP
            mus[idx, :] = np.dot(Kxxs_t, alpha_m_idx) + emulator.mean(t)

            # restore the original values for the next loop
            x[[N - 1, idx]] = x[[idx, N - 1]]
            y[[N - 1, idx]] = y[[idx, N - 1]]

            K_inv_full[[idx, N - 1], :] = K_inv_full[[N - 1, idx], :]
            K_inv_full[:, [idx, N - 1]] = K_inv_full[:, [N - 1, idx]]

        # return the jackknife cov matrix.
        cov = (N - 1.0) / N * np.cov(mus, rowvar=False)
        if mus.shape[1] == 1:
            return np.array([[cov]])  # returns float in this case
        else:
            return cov


class OriginalRecipe(Emu):
    """Emulator that emulates with only one learner that is trained on all training data. The "naive" approach. """

    def _build_gp(self, hyperparams):
        """
        Initialize the GP emulator.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        # TODO could use more of the hyperparams...
        metric = hyperparams['metric'] if 'metric' in hyperparams else {}
        kernel = self._make_kernel(metric)

        self._emulator = george.GP(kernel)
        # gp = george.GP(kernel, solver=george.HODLRSolver, nleaf=x.shape[0]+1,tol=1e-18)

        self._emulator.compute(self.x, self.yerr, sort=False)  # NOTE I'm using a modified version of george!

    def _build_skl(self, hyperparams):
        """
        Build a scikit learn emulator
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """

        if self.method in {'svr', 'krr'}:  # kernel based method
            metric = hyperparams['metric'] if 'metric' in hyperparams else {}
            kernel = self._make_kernel(metric)
            if 'metric' in hyperparams:
                del hyperparams['metric']
            # slight difference in use for these saldy
            if self.method == 'svr':
                hyperparams['kernel'] = kernel.value
            else:  # krr
                hyperparams['kernel'] = lambda x1, x2: kernel.value(np.array([x1]), np.array([x2]))

        self._emulator = self.skl_methods[self.method](**hyperparams)
        self._emulator.fit(self.x, self.y)

    def _emulate_helper(self, t, gp_errs):
        """
        Helper function that takes a dependent variable matrix and makes a prediction.
        :param t:
            Dependent variable matrix. Assumed to be in the order defined by ordered_params
        :param gp_errs:
            Whether or not to return errors in the gp case
        :return:
            mu, err (if gp_errs True). Predicted value for dependetn variable t.
            mu and err both have shape (t.shape[0])
        """
        if self.method == 'gp':
            if gp_errs:
                mu, cov = self._emulator.predict(self.y, t)
                return mu+self.y_hat, np.diag(cov)
            else:
                return self._emulator.predict(self.y, t, return_cov=False)+self.y_hat
        else:
            return self._emulator.predict(t) + self.y_hat

    def _emulator_lnlikelihood(self):
        """
        """
        # TODO docs
        assert self.method == 'gp'

        ll = self._emulator.lnlikelihood(self.y, quiet=True)

        ll = ll if np.isfinite(ll) else -1e25

        return ll, -self._emulator.grad_lnlikelihood(self.y, quiet=True)


    def train_metric(self,p0=None, **kwargs):
        """
        Train the metric parameters of the GP. Has a spotty record of working.
        Best used as used in lowDimTraining.
        If attempted to be used with an emulator that is not GP, will raise an error.
        :param kwargs:
            Kwargs that will be passed into the scipy.optimize.minimize
        :return: success: True if the training was successful.
        """

        # TODO kernel based methods may want to use this...
        assert self.method == 'gp'

        # move these outside? hm.
        def nll(p):
            # Update the kernel parameters and compute the likelihood.
            # params are log(a) and log(m)
            self._emulator.kernel[:] = p
            ll = self._emulator.lnlikelihood(self.y, quiet=True)

            # The scipy optimizer doesn't play well with infinities.
            return -ll if np.isfinite(ll) else 1e25

        # And the gradient of the objective function.
        def grad_nll(p):
            # Update the kernel parameters and compute the likelihood.
            self._emulator.kernel[:] = p
            return -self._emulator.grad_lnlikelihood(self.y, quiet=True)

        if p0 is None:
            p0 = self._emulator.kernel.vector

        results = op.minimize(nll, p0, jac=grad_nll, **kwargs)
        # results = op.minimize(nll, p0, jac=grad_nll, method='TNC', bounds =\
        #   [(np.log(0.01), np.log(10)) for i in xrange(ndim+1)],options={'maxiter':50})

        self._emulator.kernel[:] = results.x
        self._emulator.recompute()
        # self.metric = np.exp(results.x)

        return results


def get_leaves(kdtree):
    """
    Helper function for recursively retriving the leaves of a KDTree
    :param: kdtree
        instance of KDTree to recover leaves of.
    :return: leaves, a list of  numpy arrays of shape (experts, points_per_expert), of leaves.
    """
    #points_per_expert = kdtree.leafsize
    leaves_list = []
    get_leaves_helper(kdtree.tree, leaves_list)

    return [np.array(l) for l in leaves_list]


def get_leaves_helper(node, leaves):
    """
    Meta helper function. Recursively
    """
    if isinstance(node, KDTree.leafnode):
        leaves.append(node.idx)
    else:
        get_leaves_helper(node.less, leaves)
        get_leaves_helper(node.greater, leaves)


class ExtraCrispy(Emu):
    """Emulator that emulates with a mixture of expert learners rather than a single one."""

    def __init__(self, training_dir, experts, overlap=1, partition_scheme='random', **kwargs):
        """
        Similar initialization as the superclass with one additional parameter: Em_param
        :param training_dir:
            See above in EMu
        :param experts:
            number of experts to use in the mixture. Must be an integer greater than 1.
        :param overlap:
            overlap in training points between experts. For example, if overlap=2, each datapoint will be in
            2 experts. Default is 1, no overlap.
        :param kwargs:
            As in Emu
        """

        assert experts > 1 and int(experts) == experts  # no point in having less than this
        # TODO experts max value?
        # no point in having overlap the same as experts. You just have experts-many identical gps!
        assert experts > overlap > 0 and int(overlap) == overlap
        assert partition_scheme in {'kdtree', 'random'}

        self.experts = int(experts)
        self.overlap = int(overlap)
        self.partition_scheme = partition_scheme

        super(ExtraCrispy, self).__init__(training_dir, **kwargs)

    def load_training_data(self, training_dir):
        """
        Read the training data for the emulator and attach it to the object.
        :param training_dir:
            Directory where training data from trainginData is stored.
        :param fixed_params:
            Parameters to hold fixed. Only available if data in training_dir is a full hypercube, not a latin hypercube.
        :return: None
        """
        super(ExtraCrispy, self).load_training_data(training_dir)
        self.y += self.y_hat  # need to change this later

        # now, parition the data as specified by the user
        # note that ppe does not include overlap
        points_per_expert = int(1.0 * self.x.shape[0] * self.overlap / self.experts)

        try:
            assert points_per_expert > 0
        except AssertionError:
            raise AssertionError("You have too many experts!")

        _x = np.zeros((self.experts, points_per_expert, self.x.shape[1]))
        _y = np.zeros((self.experts, points_per_expert))
        _yerr = np.zeros_like(_y)

        if self.partition_scheme == 'random':
            shuffled_idxs = range(self.y.shape[0])
            np.random.shuffle(shuffled_idxs)

            # select potentially self.overlapping subets of the data for each expert
            for i in xrange(self.experts):
                _x[i, :, :] = np.roll(self.x[shuffled_idxs, :], i * points_per_expert / self.overlap, 0)[
                              :points_per_expert, :]
                _y[i, :] = np.roll(self.y[shuffled_idxs], i * points_per_expert / self.overlap, 0)[:points_per_expert]

                _yerr[i, :] = np.roll(self.yerr[shuffled_idxs], i * points_per_expert / self.overlap, 0)[
                              :points_per_expert]

        else:  # KDTree
            # whiten so all distances are the same
            normed_x = (self.x - self.x.min(axis=0)) / self.x.max(axis=0)
            normed_x[np.isnan(normed_x)] = 0.0
            kdtree = KDTree(normed_x, leafsize=points_per_expert / self.overlap)
            leaves = get_leaves(kdtree)

            prev_idx, curr_idx = 0, 0
            # If points cannot be evenly divided, there'll be some skipped ones.
            # We'll add them in at the end.
            n_missed = np.sum([(self.overlap * len(leaf) % self.experts) / self.overlap for leaf in leaves])

            missed_points = np.zeros(n_missed, dtype=int)
            missed_idx = 0

            # leaves can have different sizes, so we have to treat each leaf differently
            for i, leaf in enumerate(leaves):
                shuffled_idxs = range(leaf.shape[0])
                np.random.shuffle(shuffled_idxs)

                leaf_ppe = int(1.0 * self.overlap * leaf.shape[0] / self.experts)
                curr_idx = prev_idx + leaf_ppe

                # select potentially overlapping subets of the data for each expert
                for j in xrange(self.experts):
                    _x[j, prev_idx:curr_idx, :] = \
                        np.roll(self.x[leaf[shuffled_idxs], :], j * leaf_ppe / self.overlap, 0)[:leaf_ppe, :]
                    _y[j, prev_idx:curr_idx] = \
                        np.roll(self.y[leaf[shuffled_idxs]], j * leaf_ppe / self.overlap, 0)[:leaf_ppe]
                    _yerr[j, prev_idx:curr_idx] \
                        = np.roll(self.yerr[leaf[shuffled_idxs]], j * leaf_ppe / self.overlap, 0)[:leaf_ppe]

                prev_idx = curr_idx
                nm = (self.overlap * leaf.shape[0] % self.experts) / self.overlap
                if nm != 0:
                    missed_points[missed_idx:missed_idx + nm] = leaf[shuffled_idxs][-nm:]
                    missed_idx += nm

            # now, distribute leftover points over experts
            if n_missed > 0:
                missed_ppe = int(1.0 * n_missed * self.overlap / self.experts)

                curr_idx = prev_idx + missed_ppe

                for i in xrange(self.experts):
                    _x[i, prev_idx:curr_idx, :] = \
                        np.roll(self.x[missed_points, :], i * missed_ppe / self.overlap, 0)[:missed_ppe, :]
                    _y[j, prev_idx:curr_idx] = \
                        np.roll(self.y[missed_points], i * missed_ppe / self.overlap, 0)[:missed_ppe]
                    _yerr[j, prev_idx:curr_idx] \
                        = np.roll(self.yerr[missed_points], i * missed_ppe / self.overlap, 0)[:missed_ppe]

                # now, to cover the meta-missed ones, just fill in points until they're full
                while curr_idx != self.x.shape[1]:
                    prev_idx = curr_idx
                    curr_idx += 1
                    for j in xrange(self.experts):
                        _x[j, prev_idx:curr_idx, :] = \
                            np.roll(self.x[missed_points, :], (j + i) * missed_ppe / self.overlap, 0)[:1, :]
                        _y[j, prev_idx:curr_idx] = \
                            np.roll(self.y[missed_points], (j + i) * missed_ppe / self.overlap, 0)[:1]
                        _yerr[j, prev_idx:curr_idx] \
                            = np.roll(self.yerr[missed_points], (j + i) * missed_ppe / self.overlap, 0)[:1]

        # now attach these final versions
        self.x = _x
        self.y = _y
        self.yerr = _yerr
        self.y_hat = self.y.mean(axis=1)
        self.y -= self.y_hat.reshape((-1, 1))

    def _build_gp(self, hyperparams):
        """
        Initialize the GP emulator using an MOE model.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        if 'metric' in hyperparams:
            metric = hyperparams['metric']
            del hyperparams['metric']
        else:
            metric = {}
        kernel = self._make_kernel(metric)

        # now, make a list of emulators
        self._emulators = []

        for _x, _yerr in izip(self.x, self.yerr):
            emulator = george.GP(kernel)

            emulator.compute(_x, _yerr,**hyperparams)  # NOTE I'm using a modified version of george!

            self._emulators.append(emulator)

    def _build_skl(self, hyperparams):
        """
        Build a scikit learn emulator using a mixtrue of experts.
        :param hyperparams:
            Key word parameters for the emulator
        :return: None
        """
        skl_methods = {'gbdt': GradientBoostingRegressor, 'rf': RandomForestRegressor, \
                       'svr': SVR, 'krr': KernelRidge}

        # Same kernel concerns as above.
        if self.method in {'svr', 'krr'}:  # kernel based method
            metric = hyperparams['metric'] if 'metric' in hyperparams else {}
            kernel = self._make_kernel(metric)
            if 'metric' in hyperparams:
                del hyperparams['metric']
            if self.method == 'svr':  # slight difference in these, sadly
                hyperparams['kernel'] = kernel.value
            else:  # krr
                hyperparams['kernel'] = lambda x1, x2: kernel.value(np.array([x1]), np.array([x2]))

        self._emulators = [self.skl_methods[self.method](**hyperparams) for i in xrange(self.experts)]

        for i, (emulator, _x, _y) in enumerate(izip(self._emulators, self.x, self.y)):
            emulator.fit(_x, _y)

    def _emulate_helper(self, t, gp_errs=False):
        """
        Helper function that takes a dependent variable matrix and makes a prediction.
        :param t:
            Dependent variable matrix. Assumed to be in the order defined by ordered_params
        :param gp_errs:
            Whether or not to return errors in the gp case
        :return:
            mu, err (if gp_errs True). Predicted value for dependetn variable t.
            mu and err both have shape (npoints*self.redshift_bin_centers*self.scale_bin_centers)
        """
        mu = np.zeros((self.experts, t.shape[0]))  # experts down, t deep
        err = np.zeros_like(mu)

        for i, (emulator, _y, _yhat) in enumerate(izip(self._emulators, self.y, self.y_hat)):
            if self.method == 'gp':
                local_mu, local_cov = emulator.predict(_y, t, return_cov=True)
                local_err = np.sqrt(np.diag(local_cov))
            else:
                local_mu = emulator.predict(t)
                local_err = 1.0  # weight with this instead of the errors.

            mu[i, :] = local_mu + _yhat
            err[i, :] = local_err

        # now, combine with weighted average
        combined_var = np.reciprocal(np.sum(np.reciprocal(err ** 2), axis=0))
        combined_mu = combined_var * np.sum(np.reciprocal(err ** 2) * mu, axis=0)

        # Reshape to be consistent with my other implementation
        if not gp_errs:
            return combined_mu
        return combined_mu, np.sqrt(combined_var)

    def _emulator_lnlikelihood(self):
        """
        """
        assert self.method == 'gp'

        ll = 0
        gll = 0
        for idx, (emulator, _y) in enumerate(izip(self._emulators, self.y)):
            ll += emulator.lnlikelihood(_y, quiet=True)
            gll += emulator.grad_lnlikelihood(_y, quiet=True)

        # The scipy optimizer doesn't play well with infinities.

        ll = ll if np.isfinite(ll) else -1e25


        return ll, gll


    # TODO could make this learn the metric for other kernel based emulators...
    def train_metric(self, p0=None,  **kwargs):
        """
        Train the emulator. Has a spotty record of working. Better luck may be had with the NAMEME code.
        :param kwargs:
            Kwargs that will be passed into the scipy.optimize.minimize
        :return: success: True if the training was successful.
        """

        assert self.method == 'gp'

        # emulators is a list containing refernces to the same object. this should still work!

        # move these outside? hm.
        def nll(p):
            # Update the kernel parameters and compute the likelihood.
            # params are log(a) and log(m)
            ll = 0
            for emulator, _y in izip(self._emulators, self.y):
                emulator.kernel[:] = p
                ll += emulator.lnlikelihood(_y, quiet=True)

            # The scipy optimizer doesn't play well with infinities.
            return -ll if np.isfinite(ll) else 1e25

        # And the gradient of the objective function.
        def grad_nll(p):
            # Update the kernel parameters and compute the likelihood.
            gll = 0
            for emulator, _y in izip(self._emulators, self.y):
                emulator.kernel[:] = p
                gll += emulator.grad_lnlikelihood(_y, quiet=True)
            return -gll

        if p0 is None:
            p0 = self._emulators[0].kernel.vector

        results = op.minimize(nll, p0, jac=grad_nll, **kwargs)
        # results = op.minimize(nll, p0, jac=grad_nll, method='TNC', bounds =\
        #   [(np.log(0.01), np.log(10)) for i in xrange(ndim+1)],options={'maxiter':50})

        for emulator in self._emulators:
            emulator.kernel[:] = results.x
            emulator.recompute()

        return results.success
